{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:49:44 | INFO    | Excel: /Users/emanuelgandra/Desktop/Projetos /TesteQuant/QuantumSpreadHunters---Quantamental/data/BaseRefAtivos.xlsx\n",
      "00:49:44 | INFO    | Saída: /Users/emanuelgandra/Desktop/Projetos /TesteQuant/QuantumSpreadHunters---Quantamental/data/investing_news.parquet\n",
      "00:49:44 | INFO    | Limite de páginas: 1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Empresas:   0%|          | 0/43 [00:46<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 462\u001b[0m\n\u001b[1;32m    459\u001b[0m         log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalha fatal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 462\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[4], line 449\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    446\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalvar HTML debug: ON\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     run(\n\u001b[1;32m    450\u001b[0m         excel\u001b[38;5;241m=\u001b[39mexcel,\n\u001b[1;32m    451\u001b[0m         out_parquet\u001b[38;5;241m=\u001b[39mout_parquet,\n\u001b[1;32m    452\u001b[0m         only\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39monly,\n\u001b[1;32m    453\u001b[0m         max_pages\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_pages,\n\u001b[1;32m    454\u001b[0m         resume\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mresume,\n\u001b[1;32m    455\u001b[0m         save_html_debug\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msave_html_debug,\n\u001b[1;32m    456\u001b[0m         workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[1;32m    457\u001b[0m     )\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    459\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalha fatal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 376\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(excel, out_parquet, only, max_pages, resume, save_html_debug, workers)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    375\u001b[0m label \u001b[38;5;241m=\u001b[39m ticker_orig \u001b[38;5;129;01mor\u001b[39;00m empresa\n\u001b[0;32m--> 376\u001b[0m items \u001b[38;5;241m=\u001b[39m scrape_company(\n\u001b[1;32m    377\u001b[0m     link_news\u001b[38;5;241m=\u001b[39mlink_news,\n\u001b[1;32m    378\u001b[0m     ticker_orig\u001b[38;5;241m=\u001b[39mticker_orig \u001b[38;5;129;01mor\u001b[39;00m empresa,\n\u001b[1;32m    379\u001b[0m     sector\u001b[38;5;241m=\u001b[39msetor,\n\u001b[1;32m    380\u001b[0m     company_label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[1;32m    381\u001b[0m     polite_sleep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m    382\u001b[0m     max_pages\u001b[38;5;241m=\u001b[39mmax_pages \u001b[38;5;28;01mif\u001b[39;00m max_pages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_MAX_PAGES,\n\u001b[1;32m    383\u001b[0m     save_html_debug\u001b[38;5;241m=\u001b[39msave_html_debug,\n\u001b[1;32m    384\u001b[0m )\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items:\n\u001b[1;32m    386\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Nenhuma notícia encontrada.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 244\u001b[0m, in \u001b[0;36mscrape_company\u001b[0;34m(link_news, ticker_orig, sector, company_label, polite_sleep, max_pages, save_html_debug, html_debug_dir)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    243\u001b[0m first_url \u001b[38;5;241m=\u001b[39m page_url(url_base, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 244\u001b[0m r1 \u001b[38;5;241m=\u001b[39m fetch(first_url)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m r1:\n\u001b[1;32m    246\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Não foi possível carregar a página 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 148\u001b[0m, in \u001b[0;36mfetch\u001b[0;34m(url, max_retries, timeout)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m         r \u001b[38;5;241m=\u001b[39m SESSION\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    792\u001b[0m     conn,\n\u001b[1;32m    793\u001b[0m     method,\n\u001b[1;32m    794\u001b[0m     url,\n\u001b[1;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    804\u001b[0m )\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:1097\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1097\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1101\u001b[0m         (\n\u001b[1;32m   1102\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconn\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1108\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    612\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    613\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[1;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as du\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# ---------------- Config & Globals ----------------\n",
    "SP_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "\n",
    "DEFAULT_EXCEL = \"../data/BaseRefAtivos.xlsx\"\n",
    "DEFAULT_OUT = \"../data/investing_news.parquet\"\n",
    "DEFAULT_MAX_PAGES = 1400  # <- limite padrão para facilitar debug rápido\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/127.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Referer\": \"https://br.investing.com/\", # Informa de onde você \"veio\"\n",
    "    \"DNT\": \"1\", # Do Not Track\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "}\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update(HEADERS)\n",
    "\n",
    "RELATIVE_REGEX = re.compile(\n",
    "    r\"(?P<num>\\d+)\\s*(?P<unit>min|mins|minutos|minuto|hora|horas|dia|dias)\\s*atr[aá]s\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# ---------------- Logging ----------------\n",
    "def setup_logging(verbose: bool = False, debug: bool = False) -> None:\n",
    "    level = logging.INFO\n",
    "    if verbose or debug:\n",
    "        level = logging.DEBUG\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s | %(levelname)-7s | %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "    )\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------- Utils ----------------\n",
    "def normalize_news_url(url: str) -> str:\n",
    "    \"\"\"Garante base ...-news e remove sufixo /<n> para paginar manualmente.\"\"\"\n",
    "    url = (url or \"\").strip()\n",
    "    if not url:\n",
    "        return url\n",
    "    if url.endswith(\"-new\"):\n",
    "        url = url + \"s\"\n",
    "    url = re.sub(r\"/+$\", \"\", url)\n",
    "    m = re.search(r\"/(\\d+)$\", url)\n",
    "    if m:\n",
    "        url = url[: - (len(m.group(0)))]\n",
    "    return url\n",
    "\n",
    "def page_url(url_base: str, page: int) -> str:\n",
    "    return f\"{url_base}/{page}\"\n",
    "\n",
    "def parse_datetime_from_time_tag(time_tag) -> Optional[datetime]:\n",
    "    if time_tag is None:\n",
    "        return None\n",
    "\n",
    "    text = (time_tag.get_text() or \"\").strip().lower()\n",
    "    m = RELATIVE_REGEX.search(text)\n",
    "    if m:\n",
    "        num = int(m.group(\"num\"))\n",
    "        unit = m.group(\"unit\")\n",
    "        now_sp = datetime.now(SP_TZ)\n",
    "\n",
    "        if unit.startswith(\"min\"):\n",
    "            dt = now_sp - timedelta(minutes=num)\n",
    "        elif unit.startswith(\"hora\"):\n",
    "            dt = now_sp - timedelta(hours=num)\n",
    "        else:\n",
    "            dt = now_sp - timedelta(days=num)\n",
    "        return dt\n",
    "\n",
    "    dt_attr = time_tag.get(\"datetime\")\n",
    "    if dt_attr:\n",
    "        try:\n",
    "            dt = du.parse(dt_attr)\n",
    "            if dt.tzinfo is None:\n",
    "                dt = dt.replace(tzinfo=SP_TZ)\n",
    "            return dt.astimezone(SP_TZ)\n",
    "        except Exception as e:\n",
    "            log.debug(f\"Falha parse datetime attr '{dt_attr}': {e}\")\n",
    "\n",
    "    # fallback: parse do texto absoluto, ex.: \"12 de out. de 2025\"\n",
    "    try:\n",
    "        dt = du.parse(text, dayfirst=True, fuzzy=True, languages=[\"pt\"])\n",
    "        if dt is not None:\n",
    "            if dt.tzinfo is None:\n",
    "                dt = dt.replace(tzinfo=SP_TZ)\n",
    "            return dt.astimezone(SP_TZ)\n",
    "    except Exception as e:\n",
    "        log.debug(f\"Falha parse texto de data '{text}': {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def deduce_country_language_from_url(url: str) -> tuple[str, str]:\n",
    "    if \"br.investing.com\" in url:\n",
    "        return (\"BR\", \"pt-BR\")\n",
    "    return (\"\", \"\")\n",
    "\n",
    "def make_news_id(url: str, title: str) -> str:\n",
    "    base = (url or \"\").strip() + \"||\" + (title or \"\").strip()\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_URL, base))\n",
    "\n",
    "def html_debug_dump(html: str, company: str, page: int, outdir: Path) -> None:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    fn = outdir / f\"{safe_filename(company)}_p{page:04d}.html\"\n",
    "    try:\n",
    "        fn.write_text(html, encoding=\"utf-8\")\n",
    "        log.debug(f\"HTML salvo para debug: {fn}\")\n",
    "    except Exception as e:\n",
    "        log.warning(f\"Falha ao salvar HTML de debug: {e}\")\n",
    "\n",
    "def safe_filename(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s.strip())\n",
    "\n",
    "# ---------------- HTTP com backoff ----------------\n",
    "def fetch(url: str, max_retries: int = 5, timeout: int = 25) -> Optional[requests.Response]:\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout)\n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "            if r.status_code in (404, 410):\n",
    "                log.info(f\"HTTP {r.status_code} em {url} (provável fim).\")\n",
    "                return None\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                wait = (1.5 * (i + 1)) + random.random()\n",
    "                log.warning(f\"HTTP {r.status_code} em {url}; retry em {wait:.1f}s\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            log.warning(f\"HTTP {r.status_code} em {url}; sem retry programado.\")\n",
    "            return None\n",
    "        except requests.RequestException as e:\n",
    "            wait = (1.2 * (i + 1)) + random.random()\n",
    "            log.warning(f\"Erro rede: {e}; retry em {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "    log.error(f\"Falhou após {max_retries} tentativas: {url}\")\n",
    "    return None\n",
    "\n",
    "# ---------------- Parsing ----------------\n",
    "def parse_news_items(html: str, ticker: str, sector: str) -> List[Dict]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    ul = soup.find(\"ul\", attrs={\"data-test\": \"news-list\"})\n",
    "    if not ul:\n",
    "        log.debug(\"Ul[data-test='news-list'] não encontrada — página pode ter mudado.\")\n",
    "        return []\n",
    "\n",
    "    items: List[Dict] = []\n",
    "    arts = ul.select(\"article[data-test='article-item']\")\n",
    "    if not arts:\n",
    "        log.debug(\"Nenhum article[data-test='article-item'] encontrado nesta página.\")\n",
    "        return []\n",
    "\n",
    "    for art in arts:\n",
    "        a_title = art.select_one(\"a[data-test='article-title-link']\")\n",
    "        if not a_title:\n",
    "            log.debug(\"a[data-test='article-title-link'] ausente em um article; pulando.\")\n",
    "            continue\n",
    "\n",
    "        headline = (a_title.get_text() or \"\").strip()\n",
    "        url = a_title.get(\"href\") or \"\"\n",
    "        if url.startswith(\"/\"):\n",
    "            url = \"https://br.investing.com\" + url\n",
    "\n",
    "        a_provider = art.select_one(\"a[data-test='article-provider-link']\")\n",
    "        source = (a_provider.get_text().strip() if a_provider else \"\").strip()\n",
    "\n",
    "        t = art.select_one(\"time[data-test='article-publish-date']\")\n",
    "        dt = parse_datetime_from_time_tag(t)\n",
    "\n",
    "        country, language = deduce_country_language_from_url(url)\n",
    "        _id = make_news_id(url, headline)\n",
    "\n",
    "        items.append(\n",
    "            {\n",
    "                \"id\": _id,\n",
    "                \"datetime\": dt.isoformat() if dt else None,\n",
    "                \"source\": source,\n",
    "                \"headline\": headline,\n",
    "                \"ticker\": ticker,\n",
    "                \"sector\": sector,\n",
    "                \"country\": country,\n",
    "                \"url\": url,\n",
    "                \"language\": language,\n",
    "            }\n",
    "        )\n",
    "    return items\n",
    "\n",
    "def guess_last_page(html: str) -> Optional[int]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    pag_links = soup.select(\"div.flex.items-center.gap-2 a\")\n",
    "    nums = []\n",
    "    for a in pag_links:\n",
    "        txt = (a.get_text() or \"\").strip()\n",
    "        if txt.isdigit():\n",
    "            nums.append(int(txt))\n",
    "    return max(nums) if nums else None\n",
    "\n",
    "# ---------------- Scraper por empresa ----------------\n",
    "def scrape_company(\n",
    "    link_news: str,\n",
    "    ticker_orig: str,\n",
    "    sector: str,\n",
    "    company_label: str,\n",
    "    polite_sleep: float = 0.7,\n",
    "    max_pages: Optional[int] = DEFAULT_MAX_PAGES,\n",
    "    save_html_debug: bool = False,\n",
    "    html_debug_dir: Path = Path(\"./_html_debug\"),\n",
    ") -> List[Dict]:\n",
    "    url_base = normalize_news_url(link_news)\n",
    "    if not url_base:\n",
    "        log.warning(f\"[{company_label}] Link News vazio.\")\n",
    "        return []\n",
    "\n",
    "    first_url = page_url(url_base, 1)\n",
    "    r1 = fetch(first_url)\n",
    "    if not r1:\n",
    "        log.warning(f\"[{company_label}] Não foi possível carregar a página 1: {first_url}\")\n",
    "        return []\n",
    "\n",
    "    if save_html_debug:\n",
    "        html_debug_dump(r1.text, company_label, 1, html_debug_dir)\n",
    "\n",
    "    items = parse_news_items(r1.text, ticker_orig, sector)\n",
    "    log.info(f\"[{company_label}] p1: {len(items)} items.\")\n",
    "\n",
    "    last_page = guess_last_page(r1.text)\n",
    "    if last_page is None:\n",
    "        # fallback: iterar enquanto vier notícia (parando após X vazias seguidas)\n",
    "        page = 2\n",
    "        empty_streak = 0\n",
    "        while True:\n",
    "            if max_pages and page > max_pages:\n",
    "                log.info(f\"[{company_label}] max_pages atingido ({max_pages}).\")\n",
    "                break\n",
    "\n",
    "            url = page_url(url_base, page)\n",
    "            time.sleep(polite_sleep)\n",
    "            r = fetch(url)\n",
    "            if not r:\n",
    "                empty_streak += 1\n",
    "                log.debug(f\"[{company_label}] página {page} falhou ({empty_streak} vazias).\")\n",
    "                if empty_streak >= 3:\n",
    "                    log.info(f\"[{company_label}] 3 páginas vazias seguidas; encerrando.\")\n",
    "                    break\n",
    "                page += 1\n",
    "                continue\n",
    "\n",
    "            if save_html_debug:\n",
    "                html_debug_dump(r.text, company_label, page, html_debug_dir)\n",
    "\n",
    "            chunk = parse_news_items(r.text, ticker_orig, sector)\n",
    "            log.info(f\"[{company_label}] p{page}: {len(chunk)} items.\")\n",
    "            if not chunk:\n",
    "                empty_streak += 1\n",
    "                if empty_streak >= 3:\n",
    "                    log.info(f\"[{company_label}] 3 páginas sem itens; encerrando.\")\n",
    "                    break\n",
    "            else:\n",
    "                items.extend(chunk)\n",
    "                empty_streak = 0\n",
    "            page += 1\n",
    "    else:\n",
    "        total_pages = last_page\n",
    "        if max_pages:\n",
    "            total_pages = min(total_pages, max_pages)\n",
    "        for page in range(2, total_pages + 1):\n",
    "            url = page_url(url_base, page)\n",
    "            time.sleep(polite_sleep)\n",
    "            r = fetch(url)\n",
    "            if not r:\n",
    "                log.debug(f\"[{company_label}] Falha ao carregar p{page}.\")\n",
    "                continue\n",
    "\n",
    "            if save_html_debug:\n",
    "                html_debug_dump(r.text, company_label, page, html_debug_dir)\n",
    "\n",
    "            chunk = parse_news_items(r.text, ticker_orig, sector)\n",
    "            log.info(f\"[{company_label}] p{page}: {len(chunk)} items.\")\n",
    "            items.extend(chunk)\n",
    "\n",
    "    return items\n",
    "\n",
    "# ---------------- Execução principal ----------------\n",
    "def read_excel(excel_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    expected_cols = {\n",
    "        \"Empresa\", \"Setor\", \"Ticker BDR\", \"Ticker Original (EUA)\", \"Bolsa (EUA)\", \"Link News\",\n",
    "    }\n",
    "    missing = expected_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colunas faltantes no Excel: {sorted(missing)}\")\n",
    "    return df\n",
    "\n",
    "def merge_incremental(df_new: pd.DataFrame, out_parquet: Path) -> pd.DataFrame:\n",
    "    if out_parquet.exists():\n",
    "        df_old = pd.read_parquet(out_parquet)\n",
    "        df_all = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        df_all = df_all.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "        return df_all\n",
    "    return df_new\n",
    "\n",
    "def sort_by_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def _safe_parse_iso(x):\n",
    "        try:\n",
    "            return du.parse(x)\n",
    "        except Exception:\n",
    "            return None\n",
    "    if \"datetime\" in df.columns:\n",
    "        df[\"_dt_sort\"] = df[\"datetime\"].map(_safe_parse_iso)\n",
    "        df = df.sort_values(\"_dt_sort\", ascending=False).drop(columns=[\"_dt_sort\"])\n",
    "    return df\n",
    "\n",
    "def run(\n",
    "    excel: Path,\n",
    "    out_parquet: Path,\n",
    "    only: Optional[str],\n",
    "    max_pages: Optional[int],\n",
    "    resume: bool,\n",
    "    save_html_debug: bool,\n",
    "    workers: int,\n",
    "):\n",
    "    df_ref = read_excel(excel)\n",
    "\n",
    "    # filtro --only por ticker original OU empresa (case-insensitive, contém)\n",
    "    if only:\n",
    "        mask = (\n",
    "            df_ref[\"Ticker Original (EUA)\"].astype(str).str.contains(only, case=False, na=False) |\n",
    "            df_ref[\"Empresa\"].astype(str).str.contains(only, case=False, na=False)\n",
    "        )\n",
    "        df_ref = df_ref[mask].copy()\n",
    "        log.info(f\"Filtrando --only '{only}'. {len(df_ref)} linha(s) no Excel após filtro.\")\n",
    "\n",
    "    all_rows: List[Dict] = []\n",
    "\n",
    "    # processamento sequencial (simples e mais debugável)\n",
    "    for _, row in tqdm(df_ref.iterrows(), total=len(df_ref), desc=\"Empresas\"):\n",
    "        empresa = str(row[\"Empresa\"]).strip()\n",
    "        setor = str(row[\"Setor\"]).strip()\n",
    "        ticker_orig = str(row[\"Ticker Original (EUA)\"]).strip()\n",
    "        link_news = str(row[\"Link News\"]).strip()\n",
    "\n",
    "        if not link_news or link_news.lower() == \"nan\":\n",
    "            log.warning(f\"[{empresa}] Link News vazio; pulando.\")\n",
    "            continue\n",
    "\n",
    "        label = ticker_orig or empresa\n",
    "        items = scrape_company(\n",
    "            link_news=link_news,\n",
    "            ticker_orig=ticker_orig or empresa,\n",
    "            sector=setor,\n",
    "            company_label=label,\n",
    "            polite_sleep=0.7,\n",
    "            max_pages=max_pages if max_pages is not None else DEFAULT_MAX_PAGES,\n",
    "            save_html_debug=save_html_debug,\n",
    "        )\n",
    "        if not items:\n",
    "            log.info(f\"[{label}] Nenhuma notícia encontrada.\")\n",
    "            continue\n",
    "\n",
    "        df_company = pd.DataFrame(items).drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "\n",
    "        if resume and out_parquet.exists():\n",
    "            df_merged = merge_incremental(df_company, out_parquet)\n",
    "            df_merged = sort_by_datetime(df_merged)\n",
    "            df_merged.to_parquet(out_parquet, index=False)\n",
    "            log.info(f\"[{label}] Merge incremental -> {len(df_merged)} linhas em {out_parquet}\")\n",
    "        else:\n",
    "            all_rows.extend(df_company.to_dict(\"records\"))\n",
    "\n",
    "    # flush final quando não está em modo resume\n",
    "    if not resume and all_rows:\n",
    "        df = pd.DataFrame(all_rows).drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "        df = sort_by_datetime(df)\n",
    "        df.to_parquet(out_parquet, index=False)\n",
    "        log.info(f\"Salvo {len(df):,} notícias em {out_parquet}\")\n",
    "    elif not all_rows and not out_parquet.exists():\n",
    "        log.warning(\"Nenhuma notícia encontrada e arquivo de saída ainda não existe.\")\n",
    "\n",
    "# ---------------- CLI ----------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Scraper de notícias do Investing.com (PT-BR)\")\n",
    "    p.add_argument(\"--excel\", default=DEFAULT_EXCEL, help=\"Caminho do Excel de referência\")\n",
    "    p.add_argument(\"--out\", default=DEFAULT_OUT, help=\"Arquivo Parquet de saída\")\n",
    "    p.add_argument(\"--only\", default=None, help=\"Filtra por Ticker Original (EUA) ou Empresa (contém, case-insensitive)\")\n",
    "    p.add_argument(\"--max-pages\", type=int, default=DEFAULT_MAX_PAGES, help=\"Limite máx. de páginas por ativo (padrão 15)\")\n",
    "    p.add_argument(\"--resume\", action=\"store_true\", help=\"Mescla incremental com parquet existente (checkpoint por empresa)\")\n",
    "    p.add_argument(\"--save-html-debug\", action=\"store_true\", help=\"Salva HTML das páginas em ./_html_debug\")\n",
    "    p.add_argument(\"--workers\", type=int, default=1, help=\"(reservado) Nº de workers em paralelo (mantido sequencial por debug)\")\n",
    "    p.add_argument(\"--verbose\", action=\"store_true\", help=\"Logs detalhados (DEBUG)\")\n",
    "    p.add_argument(\"--debug\", action=\"store_true\", help=\"Equivalente a --verbose\")\n",
    "\n",
    "    # tolerar args estranhos do Jupyter/VSCode:\n",
    "    args, _unknown = p.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    # sanitiza argv quando rodar dentro de notebooks com ipykernel\n",
    "    if any(\"ipykernel\" in x for x in sys.argv):\n",
    "        # mantém os próprios args reconhecidos, graças ao parse_known_args acima\n",
    "        pass\n",
    "\n",
    "    args = parse_args()\n",
    "    setup_logging(verbose=args.verbose or args.debug, debug=args.debug)\n",
    "\n",
    "    excel = Path(args.excel)\n",
    "    out_parquet = Path(args.out)\n",
    "\n",
    "    log.info(f\"Excel: {excel.resolve()}\")\n",
    "    log.info(f\"Saída: {out_parquet.resolve()}\")\n",
    "    if args.only:\n",
    "        log.info(f\"Filtro --only: {args.only}\")\n",
    "    log.info(f\"Limite de páginas: {args.max_pages}\")\n",
    "    if args.resume:\n",
    "        log.info(\"Modo incremental: ON\")\n",
    "    if args.save_html_debug:\n",
    "        log.info(\"Salvar HTML debug: ON\")\n",
    "\n",
    "    try:\n",
    "        run(\n",
    "            excel=excel,\n",
    "            out_parquet=out_parquet,\n",
    "            only=args.only,\n",
    "            max_pages=args.max_pages,\n",
    "            resume=args.resume,\n",
    "            save_html_debug=args.save_html_debug,\n",
    "            workers=args.workers,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log.exception(f\"Falha fatal: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
