{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pairs trading BDR/ADR com Filtro de Kalman, custos e grid search.\n",
    "\n",
    "Requisitos: pandas, numpy\n",
    "pip install pandas numpy\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Utilidades gerais\n",
    "# ============================\n",
    "\n",
    "def to_datetime_index(df: pd.DataFrame, tz: Optional[str] = None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
    "    if tz:\n",
    "        df[\"timestamp\"] = df[\"timestamp\"].dt.tz_convert(tz)\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def align_series(\n",
    "    bdr: pd.DataFrame,\n",
    "    adr: pd.DataFrame,\n",
    "    price_col: str = \"close\",\n",
    "    how: str = \"inner\",\n",
    "    ffill: bool = True,\n",
    ") -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Alinha por índice temporal e retorna duas séries de preço.\"\"\"\n",
    "    b = bdr[[price_col]].rename(columns={price_col: \"bdr\"}).copy()\n",
    "    a = adr[[price_col]].rename(columns={price_col: \"adr\"}).copy()\n",
    "    df = b.join(a, how=how)\n",
    "    if ffill:\n",
    "        df = df.ffill().dropna()\n",
    "    return df[\"bdr\"], df[\"adr\"]\n",
    "\n",
    "\n",
    "def log_prices(bdr: pd.Series, adr: pd.Series) -> Tuple[pd.Series, pd.Series]:\n",
    "    return np.log(bdr.astype(float)), np.log(adr.astype(float))\n",
    "\n",
    "\n",
    "def realized_bar_seconds(index: pd.DatetimeIndex) -> float:\n",
    "    \"\"\"Duração média (em segundos) por barra, a partir do índice.\"\"\"\n",
    "    if len(index) < 2:\n",
    "        return 0.0\n",
    "    dt = (index[1:] - index[:-1]).total_seconds()\n",
    "    return float(np.median(dt))\n",
    "\n",
    "\n",
    "def max_drawdown(series: pd.Series) -> float:\n",
    "    \"\"\"Retorna MDD em unidades da própria série (mesma moeda/unidade do equity).\"\"\"\n",
    "    roll_max = series.cummax()\n",
    "    drawdown = series - roll_max\n",
    "    return float(drawdown.min())  # negativo\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Filtro de Kalman (modelo de par)\n",
    "# ============================\n",
    "\n",
    "@dataclass\n",
    "class KFConfig:\n",
    "    # ruído de processo (Q) para alfa e beta (passeio aleatório)\n",
    "    q_alpha: float = 1e-7\n",
    "    q_beta: float = 1e-6\n",
    "    # ruído de medição (R) inicial; pode ser atualizado via EWMA dos resíduos\n",
    "    r_init: float = 1e-3\n",
    "    # fator de esquecimento (EWMA) para R adaptativo\n",
    "    r_ewma_lambda: float = 0.01\n",
    "    # inicialização de P (covariância do estado)\n",
    "    p0_alpha: float = 1.0\n",
    "    p0_beta: float = 1.0\n",
    "\n",
    "\n",
    "class KalmanPair:\n",
    "    \"\"\"\n",
    "    Estado: theta_t = [alpha_t, beta_t]^T\n",
    "    Medição: y_t = alpha_t + beta_t * x_t + v_t,  v_t ~ N(0, R_t)\n",
    "    Evolução: theta_t = theta_{t-1} + w_t,        w_t ~ N(0, Q)\n",
    "    Q = diag(q_alpha, q_beta); R adaptativo por EWMA dos resíduos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: KFConfig):\n",
    "        self.cfg = cfg\n",
    "        self._fitted = False\n",
    "\n",
    "    def fit_filter(self, y: pd.Series, x: pd.Series) -> pd.DataFrame:\n",
    "        yv = y.astype(float).values\n",
    "        xv = x.astype(float).values\n",
    "        n = len(yv)\n",
    "        # Estado e covariâncias\n",
    "        theta = np.zeros((2, 1))  # [alpha, beta]\n",
    "        P = np.diag([self.cfg.p0_alpha, self.cfg.p0_beta]).astype(float)\n",
    "        Q = np.diag([self.cfg.q_alpha, self.cfg.q_beta]).astype(float)\n",
    "        R = float(self.cfg.r_init)\n",
    "\n",
    "        # buffers\n",
    "        alpha_f = np.zeros(n)\n",
    "        beta_f = np.zeros(n)\n",
    "        resid = np.zeros(n)\n",
    "        resid_var = np.zeros(n)\n",
    "        zscore = np.zeros(n)\n",
    "        p_alpha = np.zeros(n)\n",
    "        p_beta = np.zeros(n)\n",
    "\n",
    "        # EWMA para variância do resíduo\n",
    "        ewma_var = R\n",
    "\n",
    "        for t in range(n):\n",
    "            H = np.array([[1.0, xv[t]]])  # 1x2\n",
    "            # --- predição\n",
    "            theta_pred = theta  # passeio aleatório => F = I\n",
    "            P_pred = P + Q\n",
    "\n",
    "            # --- inovação\n",
    "            y_pred = float(H @ theta_pred)\n",
    "            e_t = yv[t] - y_pred\n",
    "            S_t = float(H @ P_pred @ H.T + R)  # variância da inovação\n",
    "\n",
    "            # --- ganho de Kalman\n",
    "            K_t = (P_pred @ H.T) / S_t  # 2x1\n",
    "\n",
    "            # --- atualização\n",
    "            theta = theta_pred + K_t * e_t\n",
    "            P = (np.eye(2) - K_t @ H) @ P_pred\n",
    "\n",
    "            # atualizar R (adaptativo) via EWMA da variância do resíduo\n",
    "            ewma_var = (1 - self.cfg.r_ewma_lambda) * ewma_var + self.cfg.r_ewma_lambda * (e_t ** 2)\n",
    "            R = float(max(1e-12, ewma_var))  # evitar zero\n",
    "\n",
    "            # guardas\n",
    "            alpha_f[t] = theta[0, 0]\n",
    "            beta_f[t] = theta[1, 0]\n",
    "            resid[t] = e_t\n",
    "            resid_var[t] = S_t\n",
    "            # z-score usando desvio da inovação (mais conservador)\n",
    "            zscore[t] = e_t / (math.sqrt(S_t) + 1e-12)\n",
    "            p_alpha[t] = P[0, 0]\n",
    "            p_beta[t] = P[1, 1]\n",
    "\n",
    "        out = pd.DataFrame(\n",
    "            {\n",
    "                \"alpha\": alpha_f,\n",
    "                \"beta\": beta_f,\n",
    "                \"resid\": resid,\n",
    "                \"resid_var\": resid_var,\n",
    "                \"zscore\": zscore,\n",
    "                \"p_alpha\": p_alpha,\n",
    "                \"p_beta\": p_beta,\n",
    "            },\n",
    "            index=y.index,\n",
    "        )\n",
    "        self._fitted = True\n",
    "        self.results_ = out\n",
    "        return out\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Baseline OLS estático\n",
    "# ============================\n",
    "\n",
    "def ols_hedge(y: pd.Series, x: pd.Series) -> Tuple[float, float]:\n",
    "    \"\"\"Retorna (alpha, beta) via OLS: y = alpha + beta * x + e.\"\"\"\n",
    "    X = np.vstack([np.ones_like(x.values), x.values]).T  # [1, x]\n",
    "    beta_hat = np.linalg.lstsq(X, y.values, rcond=None)[0]\n",
    "    alpha, beta = float(beta_hat[0]), float(beta_hat[1])\n",
    "    return alpha, beta\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Estratégia e backtest\n",
    "# ============================\n",
    "\n",
    "@dataclass\n",
    "class Costs:\n",
    "    cost_bdr_bps: float = 10.0    # custo (por lado) no BDR\n",
    "    cost_syn_bps: float = 2.0     # custo (por lado) no ADR \"sintético\"\n",
    "    borrow_bps_day: float = 0.0   # bps/dia sobre o valor de mercado do perna short\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StratParams:\n",
    "    z_entry: float = 1.5\n",
    "    z_stop: float = 3.5\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BacktestResult:\n",
    "    pair: str\n",
    "    params: StratParams\n",
    "    costs: Costs\n",
    "    bars: int\n",
    "    trades: int\n",
    "    winrate: float\n",
    "    expectancy_per_trade: float\n",
    "    median_hold_bars: float\n",
    "    sharpe_hourly_annualized: float\n",
    "    mdd: float\n",
    "    pnl_total: float\n",
    "    costs_total: float\n",
    "\n",
    "\n",
    "class PairStrategy:\n",
    "    \"\"\"\n",
    "    Sinal:\n",
    "      - Abrir posição quando |z| >= z_entry.\n",
    "        * z > 0: short spread => vender 1x BDR, comprar beta_t * ADR\n",
    "        * z < 0: long  spread => comprar 1x BDR, vender beta_t * ADR\n",
    "      - Fechar no cruzamento de z por 0 (take-profit) OU se |z| sobe até z_stop contra a posição.\n",
    "    PnL:\n",
    "      - dPnL ≈ pos * Δ(resid)  (com pos = +1 para long spread, -1 para short spread)\n",
    "      - Custos de entrada/saída em bps por perna e *borrow* por perna short pró-rata no tempo.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y_log: pd.Series, x_log: pd.Series, kf: pd.DataFrame):\n",
    "        self.y_log = y_log\n",
    "        self.x_log = x_log\n",
    "        self.kf = kf\n",
    "        # Spread modelado: s_t = y - alpha - beta*x\n",
    "        self.spread = y_log - kf[\"alpha\"] - kf[\"beta\"] * x_log\n",
    "        self.z = kf[\"zscore\"]\n",
    "\n",
    "    def run(self, params: StratParams, costs: Costs) -> BacktestResult:\n",
    "        idx = self.z.index\n",
    "        n = len(idx)\n",
    "        if n < 3:\n",
    "            raise ValueError(\"Série muito curta para backtest.\")\n",
    "\n",
    "        # Estado de posição: 0 = flat, +1 = long spread, -1 = short spread\n",
    "        pos = 0\n",
    "        entry_z = params.z_entry\n",
    "        stop_z = params.z_stop\n",
    "\n",
    "        # buffers\n",
    "        pos_series = np.zeros(n, dtype=int)\n",
    "        dPnL = np.zeros(n)\n",
    "        dCost = np.zeros(n)\n",
    "        open_i = None  # índice da barra de abertura para medir duração\n",
    "\n",
    "        # capital de referência: notional inicial do par (1x y + |beta0| x), em unidades de preço log ≈ escapar; \n",
    "        # Para retorno, usamos diretamente PnL da spread (unidade: \"log-preço\"), e anualizamos por hora.\n",
    "        # Para imprimir PnL \"total\", mantemos a soma absoluta (mesma unidade do spread em log). Opcionalmente, você pode escalar por preço nível.\n",
    "\n",
    "        # Para custos monetizáveis, estimamos notional por barra a partir dos níveis em preço (não log),\n",
    "        # então precisamos dos níveis originais:\n",
    "        # Reconstrói níveis aproximados (exp dos logs)\n",
    "        y_lvl = np.exp(self.y_log.values)\n",
    "        x_lvl = np.exp(self.x_log.values)\n",
    "        beta = self.kf[\"beta\"].values\n",
    "\n",
    "        # segundos por barra e fator de anualização por hora\n",
    "        sec_bar = realized_bar_seconds(idx)\n",
    "        hours_bar = max(1e-9, sec_bar / 3600.0)\n",
    "        ann_factor = math.sqrt(24.0 * 252.0 / max(1e-9, hours_bar))  # Sharpe anualizado por hora\n",
    "\n",
    "        trades = []\n",
    "        trade_pnls = []\n",
    "        holds = []\n",
    "\n",
    "        for t in range(1, n):\n",
    "            z_prev, z_now = float(self.z.iloc[t - 1]), float(self.z.iloc[t])\n",
    "            s_prev, s_now = float(self.spread.iloc[t - 1]), float(self.spread.iloc[t])\n",
    "\n",
    "            # sinais de entrada\n",
    "            if pos == 0:\n",
    "                if z_now >= entry_z:\n",
    "                    pos = -1  # short spread\n",
    "                    open_i = t\n",
    "                    # custos de abrir (duas pernas, por lado)\n",
    "                    notional_b = y_lvl[t]\n",
    "                    notional_a = abs(beta[t]) * x_lvl[t]\n",
    "                    dCost[t] -= (costs.cost_bdr_bps / 1e4) * notional_b\n",
    "                    dCost[t] -= (costs.cost_syn_bps / 1e4) * notional_a\n",
    "\n",
    "                elif z_now <= -entry_z:\n",
    "                    pos = +1  # long spread\n",
    "                    open_i = t\n",
    "                    notional_b = y_lvl[t]\n",
    "                    notional_a = abs(beta[t]) * x_lvl[t]\n",
    "                    dCost[t] -= (costs.cost_bdr_bps / 1e4) * notional_b\n",
    "                    dCost[t] -= (costs.cost_syn_bps / 1e4) * notional_a\n",
    "\n",
    "            else:\n",
    "                # stop contra a posição\n",
    "                if (pos < 0 and z_now >= stop_z) or (pos > 0 and z_now <= -stop_z):\n",
    "                    # fechar\n",
    "                    notional_b = y_lvl[t]\n",
    "                    notional_a = abs(beta[t]) * x_lvl[t]\n",
    "                    dCost[t] -= (costs.cost_bdr_bps / 1e4) * notional_b\n",
    "                    dCost[t] -= (costs.cost_syn_bps / 1e4) * notional_a\n",
    "                    # encerra trade\n",
    "                    pos = 0\n",
    "                    if open_i is not None:\n",
    "                        holds.append(t - open_i)\n",
    "                        open_i = None\n",
    "\n",
    "                # take-profit no cruzamento de zero\n",
    "                elif (pos < 0 and z_prev > 0.0 and z_now <= 0.0) or (pos > 0 and z_prev < 0.0 and z_now >= 0.0):\n",
    "                    notional_b = y_lvl[t]\n",
    "                    notional_a = abs(beta[t]) * x_lvl[t]\n",
    "                    dCost[t] -= (costs.cost_bdr_bps / 1e4) * notional_b\n",
    "                    dCost[t] -= (costs.cost_syn_bps / 1e4) * notional_a\n",
    "                    pos = 0\n",
    "                    if open_i is not None:\n",
    "                        holds.append(t - open_i)\n",
    "                        open_i = None\n",
    "\n",
    "                # custo de borrow por barra (se houver perna short)\n",
    "                if pos != 0 and costs.borrow_bps_day > 0:\n",
    "                    # Identifica pernas *short* para +1 (long spread) e -1 (short spread)\n",
    "                    # long spread: comprar BDR, vender beta*ADR  => short ADR\n",
    "                    # short spread: vender BDR, comprar beta*ADR => short BDR\n",
    "                    if pos > 0:  # short na perna ADR\n",
    "                        short_notional = abs(beta[t]) * x_lvl[t]\n",
    "                    else:        # short na perna BDR\n",
    "                        short_notional = y_lvl[t]\n",
    "                    dCost[t] -= (costs.borrow_bps_day / 1e4) * short_notional * (hours_bar / 24.0)\n",
    "\n",
    "            # PnL da mudança do spread\n",
    "            dPnL[t] = pos * (s_now - s_prev)\n",
    "            pos_series[t] = pos\n",
    "\n",
    "            # marca trades no take/stop (quando pos transiciona)\n",
    "            if pos_series[t] == 0 and pos_series[t - 1] != 0:\n",
    "                # consolidar PnL do trade recente\n",
    "                # (aqui, para simplicidade, atribuimos ao último bar)\n",
    "                pass\n",
    "\n",
    "        # Curva\n",
    "        pnl = pd.Series(dPnL, index=idx).cumsum()\n",
    "        costs_curve = pd.Series(dCost, index=idx).cumsum()\n",
    "        equity = pnl + costs_curve\n",
    "\n",
    "        # Agregações de trades (heurística: delimitar por transições de pos)\n",
    "        in_pos = False\n",
    "        last_eq = 0.0\n",
    "        eq_series = equity.values\n",
    "        for t in range(1, n):\n",
    "            if not in_pos and pos_series[t] != 0:\n",
    "                in_pos = True\n",
    "                last_eq = eq_series[t - 1]\n",
    "                trade_start = t\n",
    "            elif in_pos and pos_series[t] == 0:\n",
    "                in_pos = False\n",
    "                trade_pnls.append(eq_series[t] - last_eq)\n",
    "                trades.append((trade_start, t))\n",
    "\n",
    "        trades_count = len(trades)\n",
    "        winrate = float(np.mean(np.array(trade_pnls) > 0.0)) if trades_count > 0 else 0.0\n",
    "        expectancy = float(np.mean(trade_pnls)) if trades_count > 0 else 0.0\n",
    "        median_hold = float(np.median(holds)) if holds else 0.0\n",
    "\n",
    "        # Retornos por barra para Sharpe\n",
    "        # retorno ≈ Δequity; como não escalamos por capital fixo, Sharpe é relativo à unidade do spread;\n",
    "        # o fator de anualização por hora coloca todas as comparações no mesmo ritmo temporal.\n",
    "        rets = np.diff(equity.values, prepend=equity.values[0])\n",
    "        vol = float(np.std(rets, ddof=1))\n",
    "        mean = float(np.mean(rets))\n",
    "        sharpe_hourly = ann_factor * (mean / (vol + 1e-12))\n",
    "\n",
    "        res = BacktestResult(\n",
    "            pair=\"\",\n",
    "            params=params,\n",
    "            costs=costs,\n",
    "            bars=n,\n",
    "            trades=trades_count,\n",
    "            winrate=winrate,\n",
    "            expectancy_per_trade=expectancy,\n",
    "            median_hold_bars=median_hold,\n",
    "            sharpe_hourly_annualized=sharpe_hourly,\n",
    "            mdd=max_drawdown(equity),\n",
    "            pnl_total=float(equity.iloc[-1]),\n",
    "            costs_total=float(costs_curve.iloc[-1]),\n",
    "        )\n",
    "        return res\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Avaliações e tabelas\n",
    "# ============================\n",
    "\n",
    "@dataclass\n",
    "class PairRunSummary:\n",
    "    bdr: str\n",
    "    adr: str\n",
    "    overlap_obs: int\n",
    "    mae_teorico: float\n",
    "    mae_kf: float\n",
    "    sharpe_base: float\n",
    "    pnl_base: float\n",
    "    trades_base: int\n",
    "    sharpe_kf: float\n",
    "    pnl_kf: float\n",
    "    trades_kf: int\n",
    "    mdd_kf: float\n",
    "\n",
    "\n",
    "def compare_base_vs_kf(\n",
    "    y_log: pd.Series,\n",
    "    x_log: pd.Series,\n",
    "    kf_df: pd.DataFrame,\n",
    "    hours_bar: float,\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"Retorna (sharpe_base, pnl_base, sharpe_kf, pnl_kf) em modelo simples sem custos, para comparação rápida.\"\"\"\n",
    "    # Base OLS\n",
    "    a0, b0 = ols_hedge(y_log, x_log)\n",
    "    spread_base = y_log - a0 - b0 * x_log\n",
    "    ds = spread_base.diff().fillna(0.0).values\n",
    "    ann = math.sqrt(24.0 * 252.0 / max(1e-9, hours_bar))\n",
    "    sharpe_base = ann * (np.mean(ds) / (np.std(ds, ddof=1) + 1e-12))\n",
    "    pnl_base = float(np.nansum(ds))\n",
    "\n",
    "    # KF (sem execução/stop/custos — só como \"série\")\n",
    "    spread_kf = y_log - kf_df[\"alpha\"] - kf_df[\"beta\"] * x_log\n",
    "    dk = spread_kf.diff().fillna(0.0).values\n",
    "    sharpe_kf = ann * (np.mean(dk) / (np.std(dk, ddof=1) + 1e-12))\n",
    "    pnl_kf = float(np.nansum(dk))\n",
    "    return sharpe_base, pnl_base, sharpe_kf, pnl_kf\n",
    "\n",
    "\n",
    "def mae_residual(y_log: pd.Series, x_log: pd.Series, alpha: np.ndarray, beta: np.ndarray) -> float:\n",
    "    resid = y_log.values - alpha - beta * x_log.values\n",
    "    return float(np.mean(np.abs(resid)))\n",
    "\n",
    "\n",
    "def grid_search_for_pair(\n",
    "    bdr_name: str,\n",
    "    adr_name: str,\n",
    "    bdr_df: pd.DataFrame,\n",
    "    adr_df: pd.DataFrame,\n",
    "    kf_cfg: KFConfig,\n",
    "    Z_ENTRIES: List[float],\n",
    "    Z_STOPS: List[float],\n",
    "    COSTS_BDR_BPS: List[float],\n",
    "    COSTS_SYN_BPS: List[float],\n",
    "    BORROW_BPS_DAY: List[float],\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Retorna:\n",
    "      - topo_kf: resumo curto tipo \"Top N por Sharpe (KF)\"\n",
    "      - comparativo: base × kf por par\n",
    "      - grid: tabela longa de backtests (uma linha por combinação)\n",
    "    \"\"\"\n",
    "    bdr_df = to_datetime_index(bdr_df)\n",
    "    adr_df = to_datetime_index(adr_df)\n",
    "    y_lvl, x_lvl = align_series(bdr_df, adr_df)\n",
    "    overlap = len(y_lvl)\n",
    "\n",
    "    # logs\n",
    "    y_log, x_log = log_prices(y_lvl, x_lvl)\n",
    "\n",
    "    # Filtro de Kalman\n",
    "    kf = KalmanPair(kf_cfg).fit_filter(y_log, x_log)\n",
    "\n",
    "    # OLS \"teórico\" (estático) e MAE\n",
    "    a0, b0 = ols_hedge(y_log, x_log)\n",
    "    mae_theoretical = mae_residual(y_log, x_log, alpha=a0, beta=b0)\n",
    "    mae_kf = mae_residual(y_log, x_log, alpha=kf[\"alpha\"].values, beta=kf[\"beta\"].values)\n",
    "\n",
    "    # Infos rápidas para o comparativo\n",
    "    sec_bar = realized_bar_seconds(y_log.index)\n",
    "    hours_bar = max(1e-9, sec_bar / 3600.0)\n",
    "    sharpe_base, pnl_base, sharpe_kf_naive, pnl_kf_naive = compare_base_vs_kf(y_log, x_log, kf, hours_bar)\n",
    "\n",
    "    # Correlação de log-preços e indicador de incerteza relativa do beta\n",
    "    corr_log = float(pd.Series(y_log).corr(pd.Series(x_log)))\n",
    "    # median_ratio_diag: mediana de sqrt(Var(beta_t)) / |beta_t|\n",
    "    beta = kf[\"beta\"].values\n",
    "    p_beta = kf[\"p_beta\"].values\n",
    "    ratio_diag = np.median(np.sqrt(np.maximum(0.0, p_beta)) / (np.abs(beta) + 1e-12))\n",
    "    median_ratio_diag = float(ratio_diag)\n",
    "\n",
    "    # Backtest com execução/custos por grid\n",
    "    rows = []\n",
    "    top_rows = []  # para \"Top N por Sharpe (KF)\"\n",
    "\n",
    "    for z_in in Z_ENTRIES:\n",
    "        for z_st in Z_STOPS:\n",
    "            params = StratParams(z_entry=z_in, z_stop=z_st)\n",
    "            for cb in COSTS_BDR_BPS:\n",
    "                for cs in COSTS_SYN_BPS:\n",
    "                    for bw in BORROW_BPS_DAY:\n",
    "                        strat = PairStrategy(y_log, x_log, kf)\n",
    "                        res = strat.run(params, Costs(cb, cs, bw))\n",
    "                        rows.append(\n",
    "                            dict(\n",
    "                                pair=f\"{bdr_name}_{adr_name}\",\n",
    "                                z_entry=z_in,\n",
    "                                z_stop=z_st,\n",
    "                                cost_bdr_bps=cb,\n",
    "                                cost_syn_bps=cs,\n",
    "                                borrow_bps_day=bw,\n",
    "                                bars=res.bars,\n",
    "                                trades=res.trades,\n",
    "                                winrate=res.winrate,\n",
    "                                expectancy_per_trade=res.expectancy_per_trade,\n",
    "                                median_hold_bars=res.median_hold_bars,\n",
    "                                sharpe_hourly_annualized=res.sharpe_hourly_annualized,\n",
    "                                mdd=res.mdd,\n",
    "                                pnl_total=res.pnl_total,\n",
    "                                costs_total=res.costs_total,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        # coletar algumas combinações \"padrão\" (ex.: custos base) para o topo\n",
    "                        if abs(cs - COSTS_SYN_BPS[0]) < 1e-9 and abs(cb - COSTS_BDR_BPS[0]) < 1e-9 and abs(bw - BORROW_BPS_DAY[0]) < 1e-9:\n",
    "                            top_rows.append(\n",
    "                                dict(\n",
    "                                    bdr=bdr_name,\n",
    "                                    adr=adr_name,\n",
    "                                    overlap_obs=overlap,\n",
    "                                    mae_teorico=mae_theoretical,\n",
    "                                    mae_kf=mae_kf,\n",
    "                                    sharpe_base=sharpe_base,\n",
    "                                    pnl_base=pnl_base,\n",
    "                                    trades_base=np.nan,  # baseline aqui é série, não execução\n",
    "                                    sharpe_kf=res.sharpe_hourly_annualized,\n",
    "                                    pnl_kf=res.pnl_total,\n",
    "                                    trades_kf=res.trades,\n",
    "                                    mdd_kf=res.mdd,\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "    grid_df = pd.DataFrame(rows).sort_values([\"pair\", \"sharpe_hourly_annualized\"], ascending=[True, False])\n",
    "    top_df = pd.DataFrame(top_rows).sort_values(\"sharpe_kf\", ascending=False)\n",
    "\n",
    "    # comparativo curto (1 linha por par)\n",
    "    comp_df = pd.DataFrame(\n",
    "        [\n",
    "            dict(\n",
    "                bdr=bdr_name,\n",
    "                adr=adr_name,\n",
    "                sharpe_base=sharpe_base,\n",
    "                pnl_base=pnl_base,\n",
    "                trades_base=int(grid_df[\"trades\"].median()) if len(grid_df) else 0,\n",
    "                sharpe_kf=sharpe_kf_naive,  # nota: \"naive série\" (sem execução)\n",
    "                pnl_kf=pnl_kf_naive,\n",
    "                trades_kf=int(grid_df[\"trades\"].median()) if len(grid_df) else 0,\n",
    "                corr_log=corr_log,\n",
    "                median_ratio_diag=median_ratio_diag,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return top_df, comp_df, grid_df\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Exemplo de orquestração\n",
    "# ============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemplo mínimo:\n",
    "    # Troque 'data/AAPL34.csv' e 'data/AAPL.csv' pelos seus caminhos.\n",
    "    # CSV esperado: timestamp,close\n",
    "    try:\n",
    "        aapl34 = pd.read_csv(\"data/AAPL34.csv\")\n",
    "        aapl_adr = pd.read_csv(\"data/AAPL.csv\")\n",
    "        tsla34 = pd.read_csv(\"data/TSLA34.csv\")\n",
    "        tsla_adr = pd.read_csv(\"data/TSLA.csv\")\n",
    "    except Exception:\n",
    "        # Se não houver arquivos, crie séries dummy só para não quebrar\n",
    "        rng = pd.date_range(\"2023-01-01\", periods=1000, freq=\"H\", tz=\"UTC\")\n",
    "        base = np.cumsum(np.random.randn(len(rng)) * 0.001) + 10\n",
    "        aapl34 = pd.DataFrame({\"timestamp\": rng, \"close\": np.exp(base + 0.02*np.random.randn(len(rng)))})\n",
    "        aapl_adr = pd.DataFrame({\"timestamp\": rng, \"close\": np.exp(base + 0.02*np.random.randn(len(rng)))})\n",
    "        tsla34 = pd.DataFrame({\"timestamp\": rng, \"close\": np.exp(base + 0.03*np.random.randn(len(rng)))})\n",
    "        tsla_adr = pd.DataFrame({\"timestamp\": rng, \"close\": np.exp(base + 0.03*np.random.randn(len(rng)))})\n",
    "\n",
    "    KF_CFG = KFConfig(\n",
    "        q_alpha=1e-7,\n",
    "        q_beta=1e-6,\n",
    "        r_init=1e-3,\n",
    "        r_ewma_lambda=0.01,\n",
    "        p0_alpha=1.0,\n",
    "        p0_beta=1.0,\n",
    "    )\n",
    "\n",
    "    # Grids de parâmetros e custos (ajuste conforme seu universo)\n",
    "    Z_ENTRIES = [1.5, 2.0, 2.5]\n",
    "    Z_STOPS = [3.0, 3.5, 4.0]\n",
    "    COSTS_BDR_BPS = [10.0, 20.0]   # por lado\n",
    "    COSTS_SYN_BPS = [2.0, 5.0]     # por lado\n",
    "    BORROW_BPS_DAY = [0.0, 20.0, 40.0]\n",
    "\n",
    "    # Rode para um par\n",
    "    top_aapl, comp_aapl, grid_aapl = grid_search_for_pair(\n",
    "        \"AAPL34\",\n",
    "        \"AAPL\",\n",
    "        aapl34,\n",
    "        aapl_adr,\n",
    "        KF_CFG,\n",
    "        Z_ENTRIES,\n",
    "        Z_STOPS,\n",
    "        COSTS_BDR_BPS,\n",
    "        COSTS_SYN_BPS,\n",
    "        BORROW_BPS_DAY,\n",
    "    )\n",
    "\n",
    "    top_tsla, comp_tsla, grid_tsla = grid_search_for_pair(\n",
    "        \"TSLA34\",\n",
    "        \"TSLA\",\n",
    "        tsla34,\n",
    "        tsla_adr,\n",
    "        KF_CFG,\n",
    "        Z_ENTRIES,\n",
    "        Z_STOPS,\n",
    "        COSTS_BDR_BPS,\n",
    "        COSTS_SYN_BPS,\n",
    "        BORROW_BPS_DAY,\n",
    "    )\n",
    "\n",
    "    # Consolidados\n",
    "    print(\"\\n== Top por Sharpe (KF) ==\")\n",
    "    print(pd.concat([top_aapl, top_tsla], ignore_index=True).head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\nResumo base x KF\")\n",
    "    print(pd.concat([comp_aapl, comp_tsla], ignore_index=True).to_string(index=False))\n",
    "\n",
    "    print(\"\\nGrid (amostra):\")\n",
    "    print(pd.concat([grid_aapl, grid_tsla], ignore_index=True).head(20).to_csv(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-22 09:45:31,378 INFO Lendo BDRs…\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/11/lm50x6ys7ng_15t73h0j41fm0000gn/T/ipykernel_8891/941683138.py\", line 772, in <module>\n",
      "    main()\n",
      "  File \"/var/folders/11/lm50x6ys7ng_15t73h0j41fm0000gn/T/ipykernel_8891/941683138.py\", line 729, in main\n",
      "    bdr_map = load_bdr_sheets(Path(CFG[\"path_bdr_xlsx\"]))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/11/lm50x6ys7ng_15t73h0j41fm0000gn/T/ipykernel_8891/941683138.py\", line 116, in load_bdr_sheets\n",
      "    raw = pd.read_excel(path, sheet_name=None)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py\", line 508, in read_excel\n",
      "    data = io.parse(\n",
      "           ^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py\", line 1616, in parse\n",
      "    return self._reader.parse(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py\", line 778, in parse\n",
      "    data = self.get_sheet_data(sheet, file_rows_needed)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py\", line 615, in get_sheet_data\n",
      "    for row_number, row in enumerate(sheet.rows):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openpyxl/worksheet/_read_only.py\", line 85, in _cells_by_row\n",
      "    for idx, row in parser.parse():\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py\", line None, in parse\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ADR↔BDR — ETL + baseline teórico + Kalman (walk-forward) + backtest beta-neutral simplificado\n",
    "(versão com ajustes de robustez do KF, sizing com incerteza de β e custos de execução)\n",
    "\n",
    "Mudanças-chave (sem look-ahead):\n",
    "- ρ calibrado só no primeiro TRAIN (evita forward-bias); α_t, β_t pelo Kalman com WFA + validação interna\n",
    "- Penalização de var(Δβ) e |β-1| na escolha de (q_alpha, q_beta, r)\n",
    "- Teórico dinâmico com β CLIPPED e LAGGED; gate_kf usa sd_beta e banda de β\n",
    "- Sizing por risco: EWMA do spread + incerteza de β (lagged)\n",
    "- Custos: borrow + FX basis + slippage & comissão via turnover\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import coint\n",
    "\n",
    "# ============================ CONFIG & LOGGING ============================\n",
    "\n",
    "CFG = {\n",
    "    # dados\n",
    "    \"path_bdr_xlsx\": \"../data/Hist_BDRs.xlsx\",\n",
    "    \"path_us_xlsx\":  \"../data/Hist_Origem_BDRs.xlsx\",\n",
    "    \"path_fx_xlsx\":  \"../data/dolar.xlsx\",\n",
    "    \"out_dir\":       \"../data/output\",\n",
    "\n",
    "    # pares (BDR -> ADR)\n",
    "    \"pairs\": {\n",
    "        \"AAPL34\": \"AAPL\",\n",
    "        \"MSFT34\": \"MSFT\",\n",
    "        \"NVDC34\": \"NVDA\",\n",
    "        \"AMZO34\": \"AMZN\",\n",
    "        \"GOGL34\": \"GOOGL\",\n",
    "        \"M1TA34\": \"META\",\n",
    "        \"TSLA34\": \"TSLA\",\n",
    "        \"TSMC34\": \"TSM\",\n",
    "        \"AVGO34\": \"AVGO\",\n",
    "        \"BABA34\": \"BABA\",\n",
    "        \"JDCO34\": \"JD\",\n",
    "    },\n",
    "\n",
    "    # Alinhamento temporal\n",
    "    \"shift_bdr_base_min\": -30,   # -30min em todas as barras\n",
    "    \"shift_bdr_eod_min\":  -60,   # -60min na última barra do dia\n",
    "    \"asof_tolerance\": \"31min\",\n",
    "    \"min_overlap\": 500,\n",
    "\n",
    "    # Walk-Forward KF\n",
    "    \"wfa_train\": 252,\n",
    "    \"wfa_test\":  63,\n",
    "    # grids mais conservadores (R maior ↓ reatividade)\n",
    "    \"q_alpha_grid\": [1e-8, 1e-7, 1e-6, 1e-5],\n",
    "    \"q_beta_grid\":  [1e-8, 1e-7, 1e-6, 1e-5],\n",
    "    \"r_grid\":       [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    \"init_alpha\": 0.0,\n",
    "    \"init_beta\":  1.0,\n",
    "    \"init_p_alpha\": 1.0,\n",
    "    \"init_p_beta\":  1.0,\n",
    "\n",
    "    # Regularização/penalização na seleção do KF\n",
    "    \"kf_beta_anchor\": 1.0,\n",
    "    \"kf_w_anchor\": 0.05,     # penaliza |β-1|\n",
    "    \"kf_w_smooth\": 0.10,     # penaliza var(Δβ)\n",
    "\n",
    "    # Clip/estabilidade do β e gate\n",
    "    \"kf_beta_clip\": (0.3, 2.0),\n",
    "    \"kf_sd_beta_max\": 0.25,  # em log-space\n",
    "    \"use_kf_stability_gate\": True,\n",
    "\n",
    "    # Z-Score robusto\n",
    "    \"winsor_pct\": 0.01,\n",
    "    \"ewm_alpha\": 0.06,  # ~half-life 16 barras\n",
    "\n",
    "    # Gate de cointegração\n",
    "    \"coint_L\": 252,\n",
    "    \"coint_pval\": 0.05,\n",
    "\n",
    "    # Sinais & sizing\n",
    "    \"z_enter\": 2.0,\n",
    "    \"z_exit\": 0.75,\n",
    "    \"z_stop\": 3.0,\n",
    "    \"target_vol\": 0.08,\n",
    "    \"max_leverage\": 2.0,\n",
    "    \"uncertainty_weight\": 1.0,  # peso da incerteza de β no sizing\n",
    "\n",
    "    # Custos\n",
    "    \"slippage_bps\": 3.0,\n",
    "    \"commission_bps\": 3.0,\n",
    "    \"borrow_bps_pa\": 400.0,  # aluguel short BDR\n",
    "    \"fx_basis_bps\": 20.0,    # custo hedge FX aprox\n",
    "\n",
    "    # Diversos\n",
    "    \"plot_top_n\": None,\n",
    "}\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "log = logging.getLogger(\"pairs\")\n",
    "\n",
    "# ============================ I/O DE DADOS ============================\n",
    "\n",
    "def _ptbr_to_float(series: pd.Series) -> pd.Series:\n",
    "    try:\n",
    "        return series.astype(float)\n",
    "    except Exception:\n",
    "        return pd.to_numeric(series, errors='coerce')\n",
    "\n",
    "def load_bdr_sheets(path: Path) -> dict[str, pd.DataFrame]:\n",
    "    raw = pd.read_excel(path, sheet_name=None)\n",
    "    out = {}\n",
    "    for sheet, df in raw.items():\n",
    "        if 'Data' not in df.columns or 'Fechamento' not in df.columns:\n",
    "            continue\n",
    "        df = df.rename(columns={'Data':'datetime','Fechamento':'close_bdr','Volume Financeiro':'volume_bdr'})\n",
    "        df['datetime']  = pd.to_datetime(df['datetime'], dayfirst=True)\n",
    "        df['close_bdr'] = _ptbr_to_float(df['close_bdr'])\n",
    "        if 'volume_bdr' in df.columns:\n",
    "            df['volume_bdr'] = _ptbr_to_float(df['volume_bdr'])\n",
    "        else:\n",
    "            df['volume_bdr'] = np.nan\n",
    "        df = (df[['datetime','close_bdr','volume_bdr']]\n",
    "                .dropna(subset=['datetime','close_bdr'])\n",
    "                .sort_values('datetime')\n",
    "                .set_index('datetime'))\n",
    "        out[sheet] = df\n",
    "    return out\n",
    "\n",
    "def load_us_sheets(path: Path) -> dict[str, pd.DataFrame]:\n",
    "    raw = pd.read_excel(path, sheet_name=None)\n",
    "    out = {}\n",
    "    for sheet, df in raw.items():\n",
    "        if 'Date' not in df.columns or 'Last Price' not in df.columns:\n",
    "            continue\n",
    "        df = df.rename(columns={'Date':'datetime','Last Price':'close_us','Volume':'volume_us'})\n",
    "        df['datetime']  = pd.to_datetime(df['datetime'], dayfirst=True)\n",
    "        df['close_us']  = _ptbr_to_float(df['close_us'])\n",
    "        if 'volume_us' in df.columns:\n",
    "            df['volume_us'] = _ptbr_to_float(df['volume_us'])\n",
    "        else:\n",
    "            df['volume_us'] = np.nan\n",
    "        df = (df[['datetime','close_us','volume_us']]\n",
    "                .dropna(subset=['datetime','close_us'])\n",
    "                .sort_values('datetime')\n",
    "                .set_index('datetime'))\n",
    "        out[sheet] = df\n",
    "    return out\n",
    "\n",
    "def load_fx(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_excel(path)\n",
    "    if 'Date' not in df.columns or 'Mid Price' not in df.columns:\n",
    "        raise ValueError(\"dolar.xlsx precisa conter colunas 'Date' e 'Mid Price'.\")\n",
    "    df = df.rename(columns={'Date':'datetime','Mid Price':'usdxbrl','Volume':'volume_fx'})\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], dayfirst=True)\n",
    "    df['usdxbrl']  = _ptbr_to_float(df['usdxbrl'])\n",
    "    if 'volume_fx' in df.columns:\n",
    "        df['volume_fx'] = _ptbr_to_float(df['volume_fx'])\n",
    "    else:\n",
    "        df['volume_fx'] = np.nan\n",
    "    df = (df[['datetime','usdxbrl','volume_fx']]\n",
    "            .dropna(subset=['datetime','usdxbrl'])\n",
    "            .sort_values('datetime')\n",
    "            .set_index('datetime'))\n",
    "    return df\n",
    "\n",
    "# ============================ ALINHAMENTO & TEÓRICO ============================\n",
    "\n",
    "def shift_bdr_index_with_eod_rule(df_bdr: pd.DataFrame,\n",
    "                                  base_min: int = -30,\n",
    "                                  eod_min:  int = -60) -> pd.DataFrame:\n",
    "    out = df_bdr.copy()\n",
    "    idx = out.index\n",
    "    idx_s = pd.Series(idx, index=idx)\n",
    "    last_per_day = idx_s.groupby(idx_s.index.normalize()).transform('max')\n",
    "    is_last = (idx_s == last_per_day)\n",
    "    base_delta = pd.to_timedelta(base_min, unit='m')\n",
    "    eod_delta  = pd.to_timedelta(eod_min,  unit='m')\n",
    "    new_idx = pd.DatetimeIndex(idx + base_delta)\n",
    "    new_idx = pd.DatetimeIndex(np.where(is_last.values,\n",
    "                                        (idx + eod_delta).values,\n",
    "                                        new_idx.values))\n",
    "    out.index = new_idx\n",
    "    return out.sort_index()\n",
    "\n",
    "def nearest_join(left: pd.DataFrame, right: pd.DataFrame, tolerance: str, direction: str = \"backward\") -> pd.DataFrame:\n",
    "    l = left.sort_index().reset_index().rename(columns={'index':'datetime'})\n",
    "    r = right.sort_index().reset_index().rename(columns={'index':'datetime'})\n",
    "    m = pd.merge_asof(l, r, on='datetime',\n",
    "                      direction=direction,\n",
    "                      tolerance=pd.to_timedelta(tolerance))\n",
    "    m = m.set_index('datetime')\n",
    "    return m\n",
    "\n",
    "def theoretical_bdr_brl(px_adr_usd: pd.Series,\n",
    "                        fx_brlusd: pd.Series,\n",
    "                        rho: float) -> pd.Series:\n",
    "    return (px_adr_usd * fx_brlusd * float(rho)).rename(\"theo_bdr_brl\")\n",
    "\n",
    "# ============================ RATIO (ρ) ============================\n",
    "\n",
    "def calibrate_ratio(bdr_close: pd.Series, adr_usd: pd.Series, usdxbrl: pd.Series, method=\"ols\"):\n",
    "    denom = adr_usd * usdxbrl\n",
    "    df = pd.DataFrame({'bdr': bdr_close, 'den': denom}).dropna()\n",
    "    if df.empty: return np.nan, np.nan, 0\n",
    "    ratio_series = df['bdr'] / df['den'].replace(0, np.nan)\n",
    "    ratio_series = ratio_series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if ratio_series.empty: return np.nan, np.nan, int(len(df))\n",
    "    if method == \"median\":\n",
    "        ratio = float(np.median(ratio_series.values))\n",
    "    else:\n",
    "        num = (df['den'] * df['bdr']).sum()\n",
    "        den = (df['den'] ** 2).sum()\n",
    "        ratio = float(num / den) if den != 0 else np.nan\n",
    "    return ratio, float(ratio_series.std(ddof=1)), int(len(ratio_series))\n",
    "\n",
    "def choose_ratio_with_fallback(df_pair: pd.DataFrame, ratio_ols: float):\n",
    "    denom = df_pair['close_us'] * df_pair['usdxbrl']\n",
    "    ratio_series = (df_pair['close_bdr'] / denom).replace([np.inf,-np.inf], np.nan).dropna()\n",
    "    if ratio_series.empty:\n",
    "        return ratio_ols, np.nan, 0, \"ols\"\n",
    "    ratio_med = float(np.median(ratio_series))\n",
    "    ratio_sd  = float(ratio_series.std(ddof=1)) if len(ratio_series) > 1 else 0.0\n",
    "    if (ratio_med > 0) and (ratio_ols <= 0 or ratio_ols < ratio_med/100.0 or ratio_ols > ratio_med*100.0):\n",
    "        return ratio_med, ratio_sd, len(ratio_series), \"median_fallback\"\n",
    "    return ratio_ols, ratio_sd, len(ratio_series), \"ols\"\n",
    "\n",
    "# ============================ ESTATÍSTICA ROBUSTA ============================\n",
    "\n",
    "def robust_zscore(spread: pd.Series, ewm_alpha=0.06, winsor_pct=0.01) -> pd.Series:\n",
    "    s = spread.copy()\n",
    "    lo, hi = s.quantile([winsor_pct, 1-winsor_pct])\n",
    "    s = s.clip(lo, hi)\n",
    "    mu = s.ewm(alpha=ewm_alpha, adjust=False).mean().shift(1)\n",
    "    dev = (s - mu).abs().ewm(alpha=ewm_alpha, adjust=False).mean().shift(1)\n",
    "    eps = 1e-12\n",
    "    return ((s - mu) / (dev + eps)).rename(\"z\")\n",
    "\n",
    "# ============================ COINTEGRAÇÃO (gate) ============================\n",
    "\n",
    "def rolling_coint_gate(x: pd.Series, y: pd.Series, L=252, pval_th=0.05) -> pd.Series:\n",
    "    gate = pd.Series(False, index=y.index)\n",
    "    if len(y) < L: return gate\n",
    "    for i in range(L, len(y)):\n",
    "        _x = x.iloc[i-L:i].values\n",
    "        _y = y.iloc[i-L:i].values\n",
    "        try:\n",
    "            _, pval, _ = coint(_y, _x)\n",
    "            gate.iloc[i] = (pval < pval_th)\n",
    "        except Exception:\n",
    "            gate.iloc[i] = False\n",
    "    return gate.rename(\"coint_on\")\n",
    "\n",
    "# ============================ KALMAN (manual, 2D) ============================\n",
    "\n",
    "def kalman_alpha_beta(y: pd.Series, x: pd.Series,\n",
    "                      q_alpha=1e-5, q_beta=1e-5, r=1e-4,\n",
    "                      alpha0=0.0, beta0=1.0, p0_alpha=1.0, p0_beta=1.0):\n",
    "    \"\"\"\n",
    "    Estado 2D: y_t = α_t + β_t x_t + e_t   (tudo em LOGs)\n",
    "    Retorna DataFrame com alpha_t, beta_t, sd_alpha_t, sd_beta_t, spread_kf (inovação).\n",
    "    \"\"\"\n",
    "    y = pd.Series(y).astype(float)\n",
    "    x = pd.Series(x).astype(float)\n",
    "    idx = y.index.intersection(x.index)\n",
    "    y = y.loc[idx]; x = x.loc[idx]\n",
    "\n",
    "    theta = np.array([alpha0, beta0], dtype=float)\n",
    "    P = np.diag([p0_alpha, p0_beta])\n",
    "    Q = np.diag([q_alpha, q_beta])\n",
    "    R = np.array([[r]])\n",
    "\n",
    "    alphas, betas, sd_a, sd_b, resid = [], [], [], [], []\n",
    "\n",
    "    for t in range(len(idx)):\n",
    "        H = np.array([[1.0, float(x.iloc[t])]])\n",
    "        # Predição\n",
    "        theta_pred = theta\n",
    "        P_pred = P + Q\n",
    "        # Inovação\n",
    "        y_hat = float(H @ theta_pred)\n",
    "        nu = float(y.iloc[t] - y_hat)\n",
    "        S = float(H @ P_pred @ H.T + R)\n",
    "        K = (P_pred @ H.T) / S\n",
    "        # Atualização\n",
    "        theta = theta_pred + (K.flatten() * nu)\n",
    "        P = (np.eye(2) - K @ H) @ P_pred\n",
    "\n",
    "        alphas.append(theta[0]); betas.append(theta[1])\n",
    "        sd_a.append(np.sqrt(max(P[0,0], 0.0))); sd_b.append(np.sqrt(max(P[1,1], 0.0)))\n",
    "        resid.append(nu)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"alpha_t\": alphas,\n",
    "        \"beta_t\":  betas,\n",
    "        \"sd_alpha_t\": sd_a,\n",
    "        \"sd_beta_t\":  sd_b,\n",
    "        \"spread_kf\":  resid,\n",
    "    }, index=idx)\n",
    "    return out\n",
    "\n",
    "def fit_kf_walkforward(x_log: pd.Series, y_log: pd.Series,\n",
    "                       q_alpha_grid, q_beta_grid, r_grid,\n",
    "                       train=252, test=63,\n",
    "                       alpha0=0.0, beta0=1.0, p0_alpha=1.0, p0_beta=1.0,\n",
    "                       w_anchor=0.05, beta_anchor=1.0, w_smooth=0.10):\n",
    "    \"\"\"\n",
    "    WFA com validação interna:\n",
    "      - Para cada janela TRAIN, escolhe (q_alpha, q_beta, r) que minimiza:\n",
    "        OBJ = RMSE(residual) na *cauda* do TRAIN (últimos 20%)\n",
    "              + w_anchor * mean((β-β_anchor)^2)\n",
    "              + w_smooth * var(Δβ)\n",
    "      - Reaplica KF em TRAIN+TEST com hiperparâmetros escolhidos e coleta α,β,sd's no TEST.\n",
    "    \"\"\"\n",
    "    idx = y_log.index\n",
    "    a_full = pd.Series(index=idx, dtype=float)\n",
    "    b_full = pd.Series(index=idx, dtype=float)\n",
    "    sa_full = pd.Series(index=idx, dtype=float)\n",
    "    sb_full = pd.Series(index=idx, dtype=float)\n",
    "    s_full = pd.Series(index=idx, dtype=float)\n",
    "\n",
    "    i = train\n",
    "    while i + test <= len(idx):\n",
    "        tr = slice(idx[i-train], idx[i-1])\n",
    "        te = slice(idx[i], idx[i+test-1])\n",
    "\n",
    "        x_tr, y_tr = x_log.loc[tr], y_log.loc[tr]\n",
    "        x_te, y_te = x_log.loc[te], y_log.loc[te]\n",
    "\n",
    "        # parte de validação dentro do TRAIN (últimos 20%)\n",
    "        cut = int(round(0.8 * len(x_tr)))\n",
    "        x_val, y_val = x_tr.iloc[cut:], y_tr.iloc[cut:]\n",
    "\n",
    "        best = None\n",
    "        for qa in q_alpha_grid:\n",
    "            for qb in q_beta_grid:\n",
    "                for rr in r_grid:\n",
    "                    kf_tr = kalman_alpha_beta(\n",
    "                        y_tr, x_tr, q_alpha=qa, q_beta=qb, r=rr,\n",
    "                        alpha0=alpha0, beta0=beta0, p0_alpha=p0_alpha, p0_beta=p0_beta\n",
    "                    )\n",
    "                    # métrica na cauda do TRAIN (proxy 'quase-OOS')\n",
    "                    kf_tail = kf_tr.iloc[cut:].copy()\n",
    "                    rmse_tail = float(np.sqrt(np.nanmean((kf_tail[\"spread_kf\"].values)**2)))\n",
    "                    # regularizações de estabilidade de β\n",
    "                    beta_series = kf_tr[\"beta_t\"].values\n",
    "                    dbeta = np.diff(beta_series)\n",
    "                    smooth_pen = np.nanvar(dbeta) if len(dbeta)>1 else 0.0\n",
    "                    anchor_pen = np.nanmean((beta_series - beta_anchor)**2)\n",
    "                    obj = rmse_tail + w_anchor*anchor_pen + w_smooth*smooth_pen\n",
    "\n",
    "                    if (best is None) or (obj < best[\"obj\"]):\n",
    "                        best = {\"qa\": qa, \"qb\": qb, \"rr\": rr, \"obj\": obj}\n",
    "\n",
    "        # Refit em TRAIN+TEST com os hiperparâmetros escolhidos\n",
    "        x_block = pd.concat([x_tr, x_te], axis=0)\n",
    "        y_block = pd.concat([y_tr, y_te], axis=0)\n",
    "        kf_block = kalman_alpha_beta(\n",
    "            y_block, x_block,\n",
    "            q_alpha=best[\"qa\"], q_beta=best[\"qb\"], r=best[\"rr\"],\n",
    "            alpha0=alpha0, beta0=beta0, p0_alpha=p0_alpha, p0_beta=p0_beta\n",
    "        )\n",
    "        kk = kf_block.loc[te]\n",
    "        a_full.loc[te]  = kk[\"alpha_t\"]\n",
    "        b_full.loc[te]  = kk[\"beta_t\"]\n",
    "        sa_full.loc[te] = kk[\"sd_alpha_t\"]\n",
    "        sb_full.loc[te] = kk[\"sd_beta_t\"]\n",
    "        s_full.loc[te]  = kk[\"spread_kf\"]\n",
    "\n",
    "        i += test\n",
    "\n",
    "    return (a_full.rename(\"alpha_t\"),\n",
    "            b_full.rename(\"beta_t\"),\n",
    "            sa_full.rename(\"sd_alpha_t\"),\n",
    "            sb_full.rename(\"sd_beta_t\"),\n",
    "            s_full.rename(\"spread_kf\"))\n",
    "\n",
    "# ============================ MÉTRICAS/DIAGNÓSTICO ============================\n",
    "\n",
    "def estimate_bars_per_day_from_index(index: pd.DatetimeIndex) -> int:\n",
    "    if len(index) == 0: return 1\n",
    "    counts = pd.Series(index.normalize()).value_counts()\n",
    "    return int(np.median(counts.values))\n",
    "\n",
    "def diagnostics_quick(df_pair: pd.DataFrame):\n",
    "    x = np.log(df_pair['close_bdr'])\n",
    "    y = np.log(df_pair['close_us'] * df_pair['usdxbrl'])\n",
    "    valid = x.notna() & y.notna()\n",
    "    corr = x[valid].corr(y[valid]) if valid.any() else np.nan\n",
    "    ratio_inst = (df_pair['close_bdr'] / (df_pair['close_us'] * df_pair['usdxbrl'])).replace([np.inf,-np.inf], np.nan)\n",
    "    med_ratio = float(ratio_inst.median())\n",
    "    return {\"corr_log\": float(corr), \"median_ratio\": med_ratio, \"n\": int(valid.sum())}\n",
    "\n",
    "# ============================ BACKTEST (next-bar, custos) ============================\n",
    "\n",
    "def backtest_next_bar(df: pd.DataFrame,\n",
    "                      z_col=\"z\",\n",
    "                      theo_col=\"bdr_teo\",\n",
    "                      z_entry=2.0, z_exit=0.75, z_stop=3.0,\n",
    "                      slippage_bps=0.0, commission_bps=0.0,\n",
    "                      borrow_bps_pa=400.0, fx_basis_bps=20.0,\n",
    "                      use_risk_sizing=True, ewm_alpha=0.06,\n",
    "                      target_vol=0.08, max_leverage=2.0,\n",
    "                      gate: pd.Series | None = None,\n",
    "                      sd_beta: pd.Series | None = None,\n",
    "                      x_log: pd.Series | None = None,\n",
    "                      uncertainty_weight: float = 1.0):\n",
    "    \"\"\"\n",
    "    Backtest do spread S = BDR - Teórico (preço), execução NEXT-BAR.\n",
    "    Sizing por risco usa EWMA(std(S)) e, opcionalmente, adiciona incerteza de β (lagged):\n",
    "\n",
    "        var_eff_t = var_EWMA(S)_t  +  (theo_{t-1} * ln(X_t))^2 * Var(β_{t-1})\n",
    "\n",
    "    onde X_t = teórico estático (ADR×FX×ρ) em preço; Var(β) ≈ sd_beta^2.\n",
    "\n",
    "    Custos:\n",
    "      - slippage + comissão por turnover |Δpos|*(pxB+pxT) * (bps/1e4)\n",
    "      - aluguel short BDR (sobre notional da perna BDR)\n",
    "      - FX basis (sobre notional da perna sintética)\n",
    "    \"\"\"\n",
    "    df = df.copy().sort_index()\n",
    "\n",
    "    S   = (df[\"close_bdr\"] - df[theo_col]).rename(\"S\")\n",
    "    z   = df[z_col].copy()\n",
    "    pxB = df[\"close_bdr\"]\n",
    "    pxT = df[theo_col]\n",
    "\n",
    "    # Gate\n",
    "    if gate is not None:\n",
    "        gate = gate.reindex(df.index).fillna(False)\n",
    "    else:\n",
    "        gate = pd.Series(True, index=df.index)\n",
    "\n",
    "    # var efetiva (EWMA do spread + incerteza beta)\n",
    "    if use_risk_sizing:\n",
    "        base_var = (S - S.ewm(alpha=ewm_alpha, adjust=False).mean()).pow(2).ewm(alpha=ewm_alpha, adjust=False).mean()\n",
    "        extra = 0.0\n",
    "        if (sd_beta is not None) and (x_log is not None):\n",
    "            sd_b_lag = sd_beta.reindex(df.index).shift(1).fillna(method=\"ffill\").fillna(0.0)\n",
    "            lnX = x_log.reindex(df.index).fillna(method=\"ffill\").fillna(0.0)  # log do teórico estático\n",
    "            theo_lag = pxT.shift(1).fillna(method=\"ffill\").fillna(pxT.iloc[0])\n",
    "            extra = (theo_lag * lnX).pow(2) * (sd_b_lag.pow(2)) * float(uncertainty_weight)\n",
    "        var_eff = (base_var + extra).replace(0, np.nan)\n",
    "        vol_eff = np.sqrt(var_eff).shift(1)\n",
    "        w = (target_vol / vol_eff).clip(upper=max_leverage).fillna(0.0)\n",
    "    else:\n",
    "        w = pd.Series(target_vol, index=df.index).clip(upper=max_leverage)\n",
    "\n",
    "    # estado discreto\n",
    "    state = pd.Series(0, index=df.index, dtype=int)\n",
    "    pos = 0\n",
    "    for t, zz in z.items():\n",
    "        if not gate.loc[t]:\n",
    "            pos = 0\n",
    "        else:\n",
    "            if pos == 0:\n",
    "                if zz >  z_entry: pos = -1\n",
    "                if zz < -z_entry: pos = +1\n",
    "            else:\n",
    "                if abs(zz) > z_stop:\n",
    "                    pos = 0\n",
    "                elif abs(zz) < z_exit:\n",
    "                    pos = 0\n",
    "        state.loc[t] = pos\n",
    "\n",
    "    # posição efetiva (contínua)\n",
    "    pos_w = (state * w).rename(\"pos\")\n",
    "\n",
    "    # Execução next-bar\n",
    "    pos_exec = pos_w.shift(1).fillna(0.0)\n",
    "    dS = S.diff().fillna(0.0)\n",
    "    pnl_gross = (pos_exec * dS).rename(\"pnl_gross\")\n",
    "\n",
    "    # Turnover e custos de slippage/comissão (por mudança de posição)\n",
    "    dpos = pos_exec.diff().abs().fillna(pos_exec.abs())\n",
    "    bps_trading = (slippage_bps + commission_bps) / 1e4\n",
    "    trade_cost = (dpos * (pxB.shift(1).fillna(pxB) + pxT.shift(1).fillna(pxT)) * bps_trading).rename(\"trade_cost\")\n",
    "\n",
    "    # Custos recorrentes (diários aprox)\n",
    "    days = (df.index.to_series().diff().dt.days.fillna(0)).clip(lower=0)\n",
    "    borrow_daily = (borrow_bps_pa/1e4) / 252.0\n",
    "    fx_basis_daily = (fx_basis_bps/1e4) / 252.0\n",
    "\n",
    "    borrow_cost = (pos_exec.clip(upper=0).abs() * pxB.shift(1) * borrow_daily * (days.replace(0,1))).rename(\"borrow_cost\")\n",
    "    fx_cost     = (pos_exec.abs() * pxT.shift(1) * fx_basis_daily * (days.replace(0,1))).rename(\"fx_cost\")\n",
    "\n",
    "    pnl_net = (pnl_gross - trade_cost - borrow_cost - fx_cost).rename(\"pnl_net\")\n",
    "    equity  = pnl_net.cumsum().rename(\"equity\")\n",
    "\n",
    "    # métricas\n",
    "    bars_per_day = estimate_bars_per_day_from_index(df.index)\n",
    "    ann_factor = np.sqrt(max(bars_per_day,1) * 252.0)\n",
    "    ret = pnl_net\n",
    "    std = float(ret.std(ddof=1))\n",
    "    sharpe = (float(ret.mean()) / (std + 1e-12)) * ann_factor if std > 0 else np.nan\n",
    "    roll_max = equity.cummax()\n",
    "    mdd = float((equity - roll_max).min()) if len(equity) else np.nan\n",
    "\n",
    "    # estatística de trades\n",
    "    turns = (state.shift(1).fillna(0) != state).astype(int)\n",
    "    trade_idx = turns[turns==1].index\n",
    "    trades = int((state.abs().diff().fillna(state.abs()) > 0).sum())\n",
    "    # winrate/expectancy aproximados por janelas entre flips\n",
    "    wins = 0; losses = 0; pnl_trades = []\n",
    "    if len(trade_idx) > 1:\n",
    "        starts = trade_idx\n",
    "        stops = list(starts[1:]) + [df.index[-1]]\n",
    "        for s, e in zip(starts, stops):\n",
    "            seg = equity.loc[s:e]\n",
    "            if len(seg) >= 2:\n",
    "                dp = float(seg.iloc[-1] - seg.iloc[0])\n",
    "                pnl_trades.append(dp)\n",
    "                if dp > 0: wins += 1\n",
    "                elif dp < 0: losses += 1\n",
    "    winrate = (wins / max(wins+losses,1)) if (wins+losses)>0 else np.nan\n",
    "    expectancy = float(np.nanmedian(pnl_trades)) if len(pnl_trades)>0 else np.nan\n",
    "    # holding mediano em barras\n",
    "    hold_lengths = []\n",
    "    if len(trade_idx) > 1:\n",
    "        for s, e in zip(trade_idx, list(trade_idx[1:]) + [df.index[-1]]):\n",
    "            hold_lengths.append(max(1, (df.index.get_loc(e) - df.index.get_loc(s))))\n",
    "    med_hold = int(np.median(hold_lengths)) if hold_lengths else 0\n",
    "\n",
    "    out = pd.concat([S, z, pos_w, pnl_gross, trade_cost, borrow_cost, fx_cost, pnl_net, equity], axis=1)\n",
    "    summary = {\n",
    "        \"bars\": len(df),\n",
    "        \"trades\": trades,\n",
    "        \"winrate\": winrate,\n",
    "        \"expectancy_per_trade\": expectancy,\n",
    "        \"median_hold_bars\": med_hold,\n",
    "        \"sharpe_hourly_annualized\": sharpe,\n",
    "        \"mdd\": mdd,\n",
    "        \"pnl_total\": float(equity.iloc[-1]) if len(equity) else 0.0,\n",
    "    }\n",
    "    return out, summary\n",
    "\n",
    "# ============================ PLOTS (opcional) ============================\n",
    "\n",
    "def plot_prices(df: pd.DataFrame, name: str, outdir: Path):\n",
    "    fig = plt.figure()\n",
    "    df[['close_bdr','bdr_teo']].dropna().plot(ax=plt.gca())\n",
    "    plt.title(f\"{name} — BDR vs Teórico\")\n",
    "    plt.xlabel(\"Tempo\"); plt.ylabel(\"Preço (BRL)\")\n",
    "    fig.tight_layout(); fig.savefig(outdir / f\"{name}_prices.png\"); plt.close(fig)\n",
    "\n",
    "def plot_spread(df: pd.DataFrame, name: str, outdir: Path):\n",
    "    fig = plt.figure()\n",
    "    (df['close_bdr'] - df['bdr_teo']).dropna().plot(ax=plt.gca())\n",
    "    plt.title(f\"{name} — Spread (BDR − Teórico)\")\n",
    "    plt.xlabel(\"Tempo\"); plt.ylabel(\"Spread (BRL)\")\n",
    "    fig.tight_layout(); fig.savefig(outdir / f\"{name}_spread.png\"); plt.close(fig)\n",
    "\n",
    "def plot_zscore(df: pd.DataFrame, name: str, outdir: Path):\n",
    "    fig = plt.figure()\n",
    "    df['z'].dropna().plot(ax=plt.gca())\n",
    "    plt.title(f\"{name} — Z-Score do Spread (robusto, shift(1))\")\n",
    "    plt.xlabel(\"Tempo\"); plt.ylabel(\"Z\")\n",
    "    ax = plt.gca(); ax.axhline(0); ax.axhline(2); ax.axhline(-2)\n",
    "    fig.tight_layout(); fig.savefig(outdir / f\"{name}_zscore.png\"); plt.close(fig)\n",
    "\n",
    "# ============================ PIPELINE ============================\n",
    "\n",
    "def run_pipeline_one_pair(bdr: str, adr: str,\n",
    "                          bdr_map: dict, us_map: dict, fx_df: pd.DataFrame,\n",
    "                          out_pairs_dir: Path, plots_dir: Path):\n",
    "    # 1) Shift do BDR\n",
    "    df_bdr = shift_bdr_index_with_eod_rule(\n",
    "        bdr_map[bdr],\n",
    "        base_min=CFG[\"shift_bdr_base_min\"],\n",
    "        eod_min=CFG[\"shift_bdr_eod_min\"],\n",
    "    )\n",
    "\n",
    "    # 2) Nearest join BDR↔ADR↔FX (backward)\n",
    "    tmp = nearest_join(\n",
    "        df_bdr[['close_bdr','volume_bdr']],\n",
    "        us_map[adr][['close_us','volume_us']],\n",
    "        CFG[\"asof_tolerance\"]\n",
    "    )\n",
    "    tmp = nearest_join(tmp, fx_df[['usdxbrl','volume_fx']], CFG[\"asof_tolerance\"])\n",
    "    tmp = tmp.dropna(subset=['close_bdr','close_us','usdxbrl']).sort_index()\n",
    "\n",
    "    if len(tmp) < CFG[\"min_overlap\"]:\n",
    "        log.info(f\"[{bdr}↔{adr}] overlap insuficiente: {len(tmp)}\")\n",
    "        return None\n",
    "\n",
    "    # 3) ρ (ratio) usando apenas o primeiro TRAIN (evita look-ahead)\n",
    "    n_train = min(CFG[\"wfa_train\"], len(tmp)//3 or 1)\n",
    "    tmp_head = tmp.iloc[:n_train]\n",
    "    ratio_ols, ratio_sd_raw, n_ratio_raw = calibrate_ratio(\n",
    "        tmp_head['close_bdr'], tmp_head['close_us'], tmp_head['usdxbrl'], method=\"ols\"\n",
    "    )\n",
    "    ratio_used, ratio_sd, n_ratio, ratio_method = choose_ratio_with_fallback(tmp_head, ratio_ols)\n",
    "    tmp[\"bdr_teo\"] = theoretical_bdr_brl(tmp[\"close_us\"], tmp[\"usdxbrl\"], ratio_used)\n",
    "\n",
    "    # 4) KF Walk-Forward (α_t, β_t) sobre LOGs (com validação interna e penalizações)\n",
    "    x_log = np.log(tmp[\"bdr_teo\"])\n",
    "    y_log = np.log(tmp[\"close_bdr\"])\n",
    "\n",
    "    alpha_t, beta_t, sd_alpha_t, sd_beta_t, spread_kf_log = fit_kf_walkforward(\n",
    "        x_log, y_log,\n",
    "        q_alpha_grid=CFG[\"q_alpha_grid\"],\n",
    "        q_beta_grid=CFG[\"q_beta_grid\"],\n",
    "        r_grid=CFG[\"r_grid\"],\n",
    "        train=CFG[\"wfa_train\"],\n",
    "        test=CFG[\"wfa_test\"],\n",
    "        alpha0=CFG[\"init_alpha\"], beta0=CFG[\"init_beta\"],\n",
    "        p0_alpha=CFG[\"init_p_alpha\"], p0_beta=CFG[\"init_p_beta\"],\n",
    "        w_anchor=CFG[\"kf_w_anchor\"], beta_anchor=CFG[\"kf_beta_anchor\"], w_smooth=CFG[\"kf_w_smooth\"]\n",
    "    )\n",
    "\n",
    "    tmp[\"alpha_t\"]   = alpha_t\n",
    "    tmp[\"beta_t\"]    = beta_t\n",
    "    tmp[\"sd_beta_t\"] = sd_beta_t\n",
    "    # (lag para trading)\n",
    "    tmp[\"alpha_lag\"] = tmp[\"alpha_t\"].shift(1)\n",
    "    tmp[\"beta_lag\"]  = tmp[\"beta_t\"].shift(1)\n",
    "\n",
    "    # clip de β (lagged) para estabilidade\n",
    "    beta_lo, beta_hi = CFG[\"kf_beta_clip\"]\n",
    "    beta_lag_clipped = tmp[\"beta_lag\"].clip(lower=beta_lo, upper=beta_hi)\n",
    "\n",
    "    # teórico dinâmico (preço): exp(α) * X^β  com α/β defasados\n",
    "    tmp[\"bdr_teo_kf\"] = np.exp(tmp[\"alpha_lag\"]) * (tmp[\"bdr_teo\"] ** beta_lag_clipped)\n",
    "\n",
    "    # 5) Spreads e Z-scores\n",
    "    spread_base = (tmp[\"close_bdr\"] - tmp[\"bdr_teo\"]).rename(\"spread_base\")\n",
    "    spread_kf   = (tmp[\"close_bdr\"] - tmp[\"bdr_teo_kf\"]).rename(\"spread_kf_price\")\n",
    "    tmp[\"z\"]    = robust_zscore(spread_base, ewm_alpha=CFG[\"ewm_alpha\"], winsor_pct=CFG[\"winsor_pct\"])\n",
    "    tmp[\"z_kf\"] = robust_zscore(spread_kf,   ewm_alpha=CFG[\"ewm_alpha\"], winsor_pct=CFG[\"winsor_pct\"])\n",
    "\n",
    "    # 6) Gate de cointegração (rolling) em LOGs + gate de estabilidade do KF\n",
    "    tmp[\"coint_on\"] = rolling_coint_gate(x_log, y_log, L=CFG[\"coint_L\"], pval_th=CFG[\"coint_pval\"])\n",
    "    if CFG[\"use_kf_stability_gate\"]:\n",
    "        sd_ok = (tmp[\"sd_beta_t\"].shift(1) <= CFG[\"kf_sd_beta_max\"])\n",
    "        beta_ok = (beta_lag_clipped == tmp[\"beta_lag\"].shift(1).clip(beta_lo, beta_hi))  # True mesmo após clip\n",
    "        tmp[\"gate_kf\"] = (tmp[\"coint_on\"] & sd_ok & beta_ok).fillna(False)\n",
    "    else:\n",
    "        tmp[\"gate_kf\"] = tmp[\"coint_on\"]\n",
    "\n",
    "    # 7) Diagnósticos\n",
    "    mae_base  = (tmp[\"close_bdr\"] - tmp[\"bdr_teo\"]).abs().mean()\n",
    "    mae_kf    = (tmp[\"close_bdr\"] - tmp[\"bdr_teo_kf\"]).abs().mean()\n",
    "    diag = diagnostics_quick(tmp)\n",
    "\n",
    "    # 8) Salvar parquet do par\n",
    "    out_path = out_pairs_dir / f\"{bdr}_{adr}.parquet\"\n",
    "    tmp.reset_index().rename(columns={'index':'datetime'}).to_parquet(out_path, index=False)\n",
    "\n",
    "    # 9) Plots (baseline)\n",
    "    plot_prices(tmp, f\"{bdr}_{adr}\", plots_dir)\n",
    "    plot_spread(tmp, f\"{bdr}_{adr}\", plots_dir)\n",
    "    plot_zscore(tmp.rename(columns={\"z\":\"z\"}), f\"{bdr}_{adr}\", plots_dir)\n",
    "\n",
    "    # Baseline\n",
    "    bt_base, sum_base = backtest_next_bar(\n",
    "        tmp,\n",
    "        z_col=\"z\",\n",
    "        theo_col=\"bdr_teo\",\n",
    "        z_entry=CFG[\"z_enter\"], z_exit=CFG[\"z_exit\"], z_stop=CFG[\"z_stop\"],\n",
    "        slippage_bps=CFG[\"slippage_bps\"], commission_bps=CFG[\"commission_bps\"],\n",
    "        borrow_bps_pa=CFG[\"borrow_bps_pa\"], fx_basis_bps=CFG[\"fx_basis_bps\"],\n",
    "        use_risk_sizing=True, ewm_alpha=CFG[\"ewm_alpha\"],\n",
    "        target_vol=CFG[\"target_vol\"], max_leverage=CFG[\"max_leverage\"],\n",
    "        gate=tmp[\"coint_on\"],\n",
    "        sd_beta=None, x_log=x_log,\n",
    "        uncertainty_weight=CFG[\"uncertainty_weight\"]\n",
    "    )\n",
    "\n",
    "    # Kalman (dinâmico)\n",
    "    bt_kf, sum_kf = backtest_next_bar(\n",
    "        tmp,\n",
    "        z_col=\"z_kf\",\n",
    "        theo_col=\"bdr_teo_kf\",\n",
    "        z_entry=CFG[\"z_enter\"], z_exit=CFG[\"z_exit\"], z_stop=CFG[\"z_stop\"],\n",
    "        slippage_bps=CFG[\"slippage_bps\"], commission_bps=CFG[\"commission_bps\"],\n",
    "        borrow_bps_pa=CFG[\"borrow_bps_pa\"], fx_basis_bps=CFG[\"fx_basis_bps\"],\n",
    "        use_risk_sizing=True, ewm_alpha=CFG[\"ewm_alpha\"],\n",
    "        target_vol=CFG[\"target_vol\"], max_leverage=CFG[\"max_leverage\"],\n",
    "        gate=tmp[\"gate_kf\"],\n",
    "        sd_beta=tmp[\"sd_beta_t\"], x_log=x_log,\n",
    "        uncertainty_weight=CFG[\"uncertainty_weight\"]\n",
    "    )\n",
    "\n",
    "    # salvar curvas\n",
    "    bt_base_path = out_pairs_dir / f\"{bdr}_{adr}_bt_base.parquet\"\n",
    "    bt_kf_path   = out_pairs_dir / f\"{bdr}_{adr}_bt_kf.parquet\"\n",
    "    bt_base.reset_index().rename(columns={'index':'datetime'}).to_parquet(bt_base_path, index=False)\n",
    "    bt_kf.reset_index().rename(columns={'index':'datetime'}).to_parquet(bt_kf_path,   index=False)\n",
    "\n",
    "    # resumo\n",
    "    summary = {\n",
    "        \"bdr\": bdr, \"adr\": adr,\n",
    "        \"overlap_obs\": int(len(tmp)),\n",
    "        \"ratio_used\": float(ratio_used),\n",
    "        \"ratio_method\": ratio_method,\n",
    "        \"mae_teorico\": float(mae_base),\n",
    "        \"mae_kf\": float(mae_kf),\n",
    "        \"corr_log\": float(diag[\"corr_log\"]),\n",
    "        \"median_ratio_diag\": float(diag[\"median_ratio\"]),\n",
    "        # backtests\n",
    "        \"sharpe_base\": sum_base[\"sharpe_hourly_annualized\"],\n",
    "        \"pnl_base\": sum_base[\"pnl_total\"],\n",
    "        \"mdd_base\": sum_base[\"mdd\"],\n",
    "        \"trades_base\": sum_base[\"trades\"],\n",
    "        \"sharpe_kf\": sum_kf[\"sharpe_hourly_annualized\"],\n",
    "        \"pnl_kf\": sum_kf[\"pnl_total\"],\n",
    "        \"mdd_kf\": sum_kf[\"mdd\"],\n",
    "        \"trades_kf\": sum_kf[\"trades\"],\n",
    "    }\n",
    "\n",
    "    log.info(f\"[{bdr}_{adr}] BASE Sharpe={summary['sharpe_base']:.2f} ({summary['trades_base']} trades) | \"\n",
    "             f\"KF Sharpe={summary['sharpe_kf']:.2f} ({summary['trades_kf']} trades) | overlap={summary['overlap_obs']}\")\n",
    "    return summary, out_path\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "def main():\n",
    "    OUT_DIR = Path(CFG[\"out_dir\"])\n",
    "    (OUT_DIR / \"pairs\").mkdir(parents=True, exist_ok=True)\n",
    "    (OUT_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Carrega dados\n",
    "    log.info(\"Lendo BDRs…\")\n",
    "    bdr_map = load_bdr_sheets(Path(CFG[\"path_bdr_xlsx\"]))\n",
    "    log.info(f\"BDRs lidas: {len(bdr_map)}\")\n",
    "\n",
    "    log.info(\"Lendo ADRs…\")\n",
    "    us_map  = load_us_sheets(Path(CFG[\"path_us_xlsx\"]))\n",
    "    log.info(f\"ADRs lidas: {len(us_map)}\")\n",
    "\n",
    "    log.info(\"Lendo FX (USDBRL)…\")\n",
    "    fx_df   = load_fx(Path(CFG[\"path_fx_xlsx\"]))\n",
    "\n",
    "    # Loop de pares\n",
    "    rows_summary = []\n",
    "    for bdr, adr in CFG[\"pairs\"].items():\n",
    "        if bdr not in bdr_map:\n",
    "            log.warning(f\"BDR '{bdr}' não encontrado; pulando.\")\n",
    "            continue\n",
    "        if adr not in us_map:\n",
    "            log.warning(f\"ADR '{adr}' não encontrado; pulando.\")\n",
    "            continue\n",
    "\n",
    "        res = run_pipeline_one_pair(\n",
    "            bdr, adr, bdr_map, us_map, fx_df,\n",
    "            out_pairs_dir=(OUT_DIR / \"pairs\"),\n",
    "            plots_dir=(OUT_DIR / \"plots\")\n",
    "        )\n",
    "        if res is None:\n",
    "            continue\n",
    "        summary, _ = res\n",
    "        rows_summary.append(summary)\n",
    "\n",
    "    # Consolidado\n",
    "    if rows_summary:\n",
    "        df_sum = pd.DataFrame(rows_summary).sort_values(\"sharpe_kf\", ascending=False)\n",
    "        df_sum.to_csv(OUT_DIR / \"summary_pairs.csv\", index=False, float_format=\"%.8f\")\n",
    "        log.info(\"\\n== Top 10 por Sharpe (KF) ==\")\n",
    "        cols = [\"bdr\",\"adr\",\"overlap_obs\",\"mae_teorico\",\"mae_kf\",\n",
    "                \"sharpe_base\",\"pnl_base\",\"trades_base\",\n",
    "                \"sharpe_kf\",\"pnl_kf\",\"trades_kf\",\"mdd_kf\"]\n",
    "        log.info(\"\\n\" + df_sum[cols].head(10).to_string(index=False))\n",
    "    else:\n",
    "        log.warning(\"Nenhum par válido processado.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ADR↔BDR — ETL + baseline teórico + Kalman (walk-forward) + backtest beta-neutral simplificado\n",
    "(versão com ajustes de robustez do KF, sizing com incerteza de β e custos de execução)\n",
    "\n",
    "Mudanças-chave (sem look-ahead):\n",
    "- ρ calibrado só no primeiro TRAIN (evita forward-bias); α_t, β_t pelo Kalman com WFA + validação interna\n",
    "- Penalização de var(Δβ) e |β-1| na escolha de (q_alpha, q_beta, r)\n",
    "- Teórico dinâmico com β CLIPPED e LAGGED; gate_kf usa sd_beta e banda de β\n",
    "- Sizing por risco: EWMA do spread + incerteza de β (lagged)\n",
    "- Custos: borrow + FX basis + slippage & comissão via turnover\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import coint\n",
    "\n",
    "# ============================ CONFIG & LOGGING ============================\n",
    "\n",
    "CFG = {\n",
    "    # dados\n",
    "    \"path_bdr_xlsx\": \"../data/Hist_BDRs.xlsx\",\n",
    "    \"path_us_xlsx\":  \"../data/Hist_Origem_BDRs.xlsx\",\n",
    "    \"path_fx_xlsx\":  \"../data/dolar.xlsx\",\n",
    "    \"out_dir\":       \"../data/output\",\n",
    "\n",
    "    # pares (BDR -> ADR)\n",
    "    \"pairs\": {\n",
    "        \"AAPL34\": \"AAPL\",\n",
    "        \"MSFT34\": \"MSFT\",\n",
    "        \"NVDC34\": \"NVDA\",\n",
    "        \"AMZO34\": \"AMZN\",\n",
    "        \"GOGL34\": \"GOOGL\",\n",
    "        \"M1TA34\": \"META\",\n",
    "        \"TSLA34\": \"TSLA\",\n",
    "        \"TSMC34\": \"TSM\",\n",
    "        \"AVGO34\": \"AVGO\",\n",
    "        \"BABA34\": \"BABA\",\n",
    "        \"JDCO34\": \"JD\",\n",
    "    },\n",
    "\n",
    "    # Alinhamento temporal\n",
    "    \"shift_bdr_base_min\": -30,   # -30min em todas as barras\n",
    "    \"shift_bdr_eod_min\":  -60,   # -60min na última barra do dia\n",
    "    \"asof_tolerance\": \"31min\",\n",
    "    \"min_overlap\": 500,\n",
    "\n",
    "    # Walk-Forward KF\n",
    "    \"wfa_train\": 252,\n",
    "    \"wfa_test\":  63,\n",
    "    # grids mais conservadores (R maior ↓ reatividade)\n",
    "    \"q_alpha_grid\": [1e-8, 1e-7, 1e-6, 1e-5],\n",
    "    \"q_beta_grid\":  [1e-8, 1e-7, 1e-6, 1e-5],\n",
    "    \"r_grid\":       [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    \"init_alpha\": 0.0,\n",
    "    \"init_beta\":  1.0,\n",
    "    \"init_p_alpha\": 1.0,\n",
    "    \"init_p_beta\":  1.0,\n",
    "\n",
    "    # Regularização/penalização na seleção do KF\n",
    "    \"kf_beta_anchor\": 1.0,\n",
    "    \"kf_w_anchor\": 0.05,     # penaliza |β-1|\n",
    "    \"kf_w_smooth\": 0.10,     # penaliza var(Δβ)\n",
    "\n",
    "    # Clip/estabilidade do β e gate\n",
    "    \"kf_beta_clip\": (0.3, 2.0),\n",
    "    \"kf_sd_beta_max\": 0.25,  # em log-space\n",
    "    \"use_kf_stability_gate\": True,\n",
    "\n",
    "    # Z-Score robusto\n",
    "    \"winsor_pct\": 0.01,\n",
    "    \"ewm_alpha\": 0.06,  # ~half-life 16 barras\n",
    "\n",
    "    # Gate de cointegração\n",
    "    \"coint_L\": 252,\n",
    "    \"coint_pval\": 0.05,\n",
    "\n",
    "    # Sinais & sizing\n",
    "    \"z_enter\": 2.0,\n",
    "    \"z_exit\": 0.75,\n",
    "    \"z_stop\": 3.0,\n",
    "    \"target_vol\": 0.08,\n",
    "    \"max_leverage\": 2.0,\n",
    "    \"uncertainty_weight\": 1.0,  # peso da incerteza de β no sizing\n",
    "\n",
    "    # Custos\n",
    "    \"slippage_bps\": 3.0,\n",
    "    \"commission_bps\": 3.0,\n",
    "    \"borrow_bps_pa\": 400.0,  # aluguel short BDR\n",
    "    \"fx_basis_bps\": 20.0,    # custo hedge FX aprox\n",
    "\n",
    "    # Diversos\n",
    "    \"plot_top_n\": None,\n",
    "    \n",
    "    # === HMM (gate de regimes) ===\n",
    "    \"use_hmm_gate\": True,\n",
    "    \"hmm_obs\": \"dspread_kf\",   # opções: \"dspread_kf\", \"spread_kf\", \"z_kf\", \"spread\", \"z\"\n",
    "    \"hmm_n_states\": 2,\n",
    "    \"hmm_max_iter\": 100,\n",
    "    \"hmm_tol\": 1e-4,\n",
    "    \"hmm_p_stay_init\": 0.95,   # matriz de transição inicial quase diagonal\n",
    "    \"hmm_p_threshold\": 0.6,    # P(MR) mínima para liberar trade\n",
    "\n",
    "}\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "log = logging.getLogger(\"pairs\")\n",
    "\n",
    "# ============================ I/O DE DADOS ============================\n",
    "\n",
    "def _ptbr_to_float(series: pd.Series) -> pd.Series:\n",
    "    try:\n",
    "        return series.astype(float)\n",
    "    except Exception:\n",
    "        return pd.to_numeric(series, errors='coerce')\n",
    "\n",
    "def load_bdr_sheets(path: Path) -> dict[str, pd.DataFrame]:\n",
    "    raw = pd.read_excel(path, sheet_name=None)\n",
    "    out = {}\n",
    "    for sheet, df in raw.items():\n",
    "        if 'Data' not in df.columns or 'Fechamento' not in df.columns:\n",
    "            continue\n",
    "        df = df.rename(columns={'Data':'datetime','Fechamento':'close_bdr','Volume Financeiro':'volume_bdr'})\n",
    "        df['datetime']  = pd.to_datetime(df['datetime'], dayfirst=True)\n",
    "        df['close_bdr'] = _ptbr_to_float(df['close_bdr'])\n",
    "        if 'volume_bdr' in df.columns:\n",
    "            df['volume_bdr'] = _ptbr_to_float(df['volume_bdr'])\n",
    "        else:\n",
    "            df['volume_bdr'] = np.nan\n",
    "        df = (df[['datetime','close_bdr','volume_bdr']]\n",
    "                .dropna(subset=['datetime','close_bdr'])\n",
    "                .sort_values('datetime')\n",
    "                .set_index('datetime'))\n",
    "        out[sheet] = df\n",
    "    return out\n",
    "\n",
    "def load_us_sheets(path: Path) -> dict[str, pd.DataFrame]:\n",
    "    raw = pd.read_excel(path, sheet_name=None)\n",
    "    out = {}\n",
    "    for sheet, df in raw.items():\n",
    "        if 'Date' not in df.columns or 'Last Price' not in df.columns:\n",
    "            continue\n",
    "        df = df.rename(columns={'Date':'datetime','Last Price':'close_us','Volume':'volume_us'})\n",
    "        df['datetime']  = pd.to_datetime(df['datetime'], dayfirst=True)\n",
    "        df['close_us']  = _ptbr_to_float(df['close_us'])\n",
    "        if 'volume_us' in df.columns:\n",
    "            df['volume_us'] = _ptbr_to_float(df['volume_us'])\n",
    "        else:\n",
    "            df['volume_us'] = np.nan\n",
    "        df = (df[['datetime','close_us','volume_us']]\n",
    "                .dropna(subset=['datetime','close_us'])\n",
    "                .sort_values('datetime')\n",
    "                .set_index('datetime'))\n",
    "        out[sheet] = df\n",
    "    return out\n",
    "\n",
    "def load_fx(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_excel(path)\n",
    "    if 'Date' not in df.columns or 'Mid Price' not in df.columns:\n",
    "        raise ValueError(\"dolar.xlsx precisa conter colunas 'Date' e 'Mid Price'.\")\n",
    "    df = df.rename(columns={'Date':'datetime','Mid Price':'usdxbrl','Volume':'volume_fx'})\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], dayfirst=True)\n",
    "    df['usdxbrl']  = _ptbr_to_float(df['usdxbrl'])\n",
    "    if 'volume_fx' in df.columns:\n",
    "        df['volume_fx'] = _ptbr_to_float(df['volume_fx'])\n",
    "    else:\n",
    "        df['volume_fx'] = np.nan\n",
    "    df = (df[['datetime','usdxbrl','volume_fx']]\n",
    "            .dropna(subset=['datetime','usdxbrl'])\n",
    "            .sort_values('datetime')\n",
    "            .set_index('datetime'))\n",
    "    return df\n",
    "\n",
    "# ============================ ALINHAMENTO & TEÓRICO ============================\n",
    "\n",
    "def shift_bdr_index_with_eod_rule(df_bdr: pd.DataFrame,\n",
    "                                  base_min: int = -30,\n",
    "                                  eod_min:  int = -60) -> pd.DataFrame:\n",
    "    out = df_bdr.copy()\n",
    "    idx = out.index\n",
    "    idx_s = pd.Series(idx, index=idx)\n",
    "    last_per_day = idx_s.groupby(idx_s.index.normalize()).transform('max')\n",
    "    is_last = (idx_s == last_per_day)\n",
    "    base_delta = pd.to_timedelta(base_min, unit='m')\n",
    "    eod_delta  = pd.to_timedelta(eod_min,  unit='m')\n",
    "    new_idx = pd.DatetimeIndex(idx + base_delta)\n",
    "    new_idx = pd.DatetimeIndex(np.where(is_last.values,\n",
    "                                        (idx + eod_delta).values,\n",
    "                                        new_idx.values))\n",
    "    out.index = new_idx\n",
    "    return out.sort_index()\n",
    "\n",
    "def nearest_join(left: pd.DataFrame, right: pd.DataFrame, tolerance: str, direction: str = \"backward\") -> pd.DataFrame:\n",
    "    l = left.sort_index().reset_index().rename(columns={'index':'datetime'})\n",
    "    r = right.sort_index().reset_index().rename(columns={'index':'datetime'})\n",
    "    m = pd.merge_asof(l, r, on='datetime',\n",
    "                      direction=direction,\n",
    "                      tolerance=pd.to_timedelta(tolerance))\n",
    "    m = m.set_index('datetime')\n",
    "    return m\n",
    "\n",
    "def theoretical_bdr_brl(px_adr_usd: pd.Series,\n",
    "                        fx_brlusd: pd.Series,\n",
    "                        rho: float) -> pd.Series:\n",
    "    return (px_adr_usd * fx_brlusd * float(rho)).rename(\"theo_bdr_brl\")\n",
    "\n",
    "# ============================ RATIO (ρ) ============================\n",
    "\n",
    "def calibrate_ratio(bdr_close: pd.Series, adr_usd: pd.Series, usdxbrl: pd.Series, method=\"ols\"):\n",
    "    denom = adr_usd * usdxbrl\n",
    "    df = pd.DataFrame({'bdr': bdr_close, 'den': denom}).dropna()\n",
    "    if df.empty: return np.nan, np.nan, 0\n",
    "    ratio_series = df['bdr'] / df['den'].replace(0, np.nan)\n",
    "    ratio_series = ratio_series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if ratio_series.empty: return np.nan, np.nan, int(len(df))\n",
    "    if method == \"median\":\n",
    "        ratio = float(np.median(ratio_series.values))\n",
    "    else:\n",
    "        num = (df['den'] * df['bdr']).sum()\n",
    "        den = (df['den'] ** 2).sum()\n",
    "        ratio = float(num / den) if den != 0 else np.nan\n",
    "    return ratio, float(ratio_series.std(ddof=1)), int(len(ratio_series))\n",
    "\n",
    "def choose_ratio_with_fallback(df_pair: pd.DataFrame, ratio_ols: float):\n",
    "    denom = df_pair['close_us'] * df_pair['usdxbrl']\n",
    "    ratio_series = (df_pair['close_bdr'] / denom).replace([np.inf,-np.inf], np.nan).dropna()\n",
    "    if ratio_series.empty:\n",
    "        return ratio_ols, np.nan, 0, \"ols\"\n",
    "    ratio_med = float(np.median(ratio_series))\n",
    "    ratio_sd  = float(ratio_series.std(ddof=1)) if len(ratio_series) > 1 else 0.0\n",
    "    if (ratio_med > 0) and (ratio_ols <= 0 or ratio_ols < ratio_med/100.0 or ratio_ols > ratio_med*100.0):\n",
    "        return ratio_med, ratio_sd, len(ratio_series), \"median_fallback\"\n",
    "    return ratio_ols, ratio_sd, len(ratio_series), \"ols\"\n",
    "\n",
    "# ============================ ESTATÍSTICA ROBUSTA ============================\n",
    "\n",
    "def robust_zscore(spread: pd.Series, ewm_alpha=0.06, winsor_pct=0.01) -> pd.Series:\n",
    "    s = spread.copy()\n",
    "    lo, hi = s.quantile([winsor_pct, 1-winsor_pct])\n",
    "    s = s.clip(lo, hi)\n",
    "    mu = s.ewm(alpha=ewm_alpha, adjust=False).mean().shift(1)\n",
    "    dev = (s - mu).abs().ewm(alpha=ewm_alpha, adjust=False).mean().shift(1)\n",
    "    eps = 1e-12\n",
    "    return ((s - mu) / (dev + eps)).rename(\"z\")\n",
    "\n",
    "# ============================ COINTEGRAÇÃO (gate) ============================\n",
    "\n",
    "def rolling_coint_gate(x: pd.Series, y: pd.Series, L=252, pval_th=0.05) -> pd.Series:\n",
    "    gate = pd.Series(False, index=y.index)\n",
    "    if len(y) < L: return gate\n",
    "    for i in range(L, len(y)):\n",
    "        _x = x.iloc[i-L:i].values\n",
    "        _y = y.iloc[i-L:i].values\n",
    "        try:\n",
    "            _, pval, _ = coint(_y, _x)\n",
    "            gate.iloc[i] = (pval < pval_th)\n",
    "        except Exception:\n",
    "            gate.iloc[i] = False\n",
    "    return gate.rename(\"coint_on\")\n",
    "\n",
    "# ============================ KALMAN (manual, 2D) ============================\n",
    "\n",
    "def kalman_alpha_beta(y: pd.Series, x: pd.Series,\n",
    "                      q_alpha=1e-5, q_beta=1e-5, r=1e-4,\n",
    "                      alpha0=0.0, beta0=1.0, p0_alpha=1.0, p0_beta=1.0):\n",
    "    \"\"\"\n",
    "    Estado 2D: y_t = α_t + β_t x_t + e_t   (tudo em LOGs)\n",
    "    Retorna DataFrame com alpha_t, beta_t, sd_alpha_t, sd_beta_t, spread_kf (inovação).\n",
    "    \"\"\"\n",
    "    y = pd.Series(y).astype(float)\n",
    "    x = pd.Series(x).astype(float)\n",
    "    idx = y.index.intersection(x.index)\n",
    "    y = y.loc[idx]; x = x.loc[idx]\n",
    "\n",
    "    theta = np.array([alpha0, beta0], dtype=float)\n",
    "    P = np.diag([p0_alpha, p0_beta])\n",
    "    Q = np.diag([q_alpha, q_beta])\n",
    "    R = np.array([[r]])\n",
    "\n",
    "    alphas, betas, sd_a, sd_b, resid = [], [], [], [], []\n",
    "\n",
    "    for t in range(len(idx)):\n",
    "        H = np.array([[1.0, float(x.iloc[t])]])\n",
    "        # Predição\n",
    "        theta_pred = theta\n",
    "        P_pred = P + Q\n",
    "        # Inovação\n",
    "        y_hat = float(H @ theta_pred)\n",
    "        nu = float(y.iloc[t] - y_hat)\n",
    "        S = float(H @ P_pred @ H.T + R)\n",
    "        K = (P_pred @ H.T) / S\n",
    "        # Atualização\n",
    "        theta = theta_pred + (K.flatten() * nu)\n",
    "        P = (np.eye(2) - K @ H) @ P_pred\n",
    "\n",
    "        alphas.append(theta[0]); betas.append(theta[1])\n",
    "        sd_a.append(np.sqrt(max(P[0,0], 0.0))); sd_b.append(np.sqrt(max(P[1,1], 0.0)))\n",
    "        resid.append(nu)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"alpha_t\": alphas,\n",
    "        \"beta_t\":  betas,\n",
    "        \"sd_alpha_t\": sd_a,\n",
    "        \"sd_beta_t\":  sd_b,\n",
    "        \"spread_kf\":  resid,\n",
    "    }, index=idx)\n",
    "    return out\n",
    "\n",
    "def fit_kf_walkforward(x_log: pd.Series, y_log: pd.Series,\n",
    "                       q_alpha_grid, q_beta_grid, r_grid,\n",
    "                       train=252, test=63,\n",
    "                       alpha0=0.0, beta0=1.0, p0_alpha=1.0, p0_beta=1.0,\n",
    "                       w_anchor=0.05, beta_anchor=1.0, w_smooth=0.10):\n",
    "    \"\"\"\n",
    "    WFA com validação interna:\n",
    "      - Para cada janela TRAIN, escolhe (q_alpha, q_beta, r) que minimiza:\n",
    "        OBJ = RMSE(residual) na *cauda* do TRAIN (últimos 20%)\n",
    "              + w_anchor * mean((β-β_anchor)^2)\n",
    "              + w_smooth * var(Δβ)\n",
    "      - Reaplica KF em TRAIN+TEST com hiperparâmetros escolhidos e coleta α,β,sd's no TEST.\n",
    "    \"\"\"\n",
    "    idx = y_log.index\n",
    "    a_full = pd.Series(index=idx, dtype=float)\n",
    "    b_full = pd.Series(index=idx, dtype=float)\n",
    "    sa_full = pd.Series(index=idx, dtype=float)\n",
    "    sb_full = pd.Series(index=idx, dtype=float)\n",
    "    s_full = pd.Series(index=idx, dtype=float)\n",
    "\n",
    "    i = train\n",
    "    while i + test <= len(idx):\n",
    "        tr = slice(idx[i-train], idx[i-1])\n",
    "        te = slice(idx[i], idx[i+test-1])\n",
    "\n",
    "        x_tr, y_tr = x_log.loc[tr], y_log.loc[tr]\n",
    "        x_te, y_te = x_log.loc[te], y_log.loc[te]\n",
    "\n",
    "        # parte de validação dentro do TRAIN (últimos 20%)\n",
    "        cut = int(round(0.8 * len(x_tr)))\n",
    "        x_val, y_val = x_tr.iloc[cut:], y_tr.iloc[cut:]\n",
    "\n",
    "        best = None\n",
    "        for qa in q_alpha_grid:\n",
    "            for qb in q_beta_grid:\n",
    "                for rr in r_grid:\n",
    "                    kf_tr = kalman_alpha_beta(\n",
    "                        y_tr, x_tr, q_alpha=qa, q_beta=qb, r=rr,\n",
    "                        alpha0=alpha0, beta0=beta0, p0_alpha=p0_alpha, p0_beta=p0_beta\n",
    "                    )\n",
    "                    # métrica na cauda do TRAIN (proxy 'quase-OOS')\n",
    "                    kf_tail = kf_tr.iloc[cut:].copy()\n",
    "                    rmse_tail = float(np.sqrt(np.nanmean((kf_tail[\"spread_kf\"].values)**2)))\n",
    "                    # regularizações de estabilidade de β\n",
    "                    beta_series = kf_tr[\"beta_t\"].values\n",
    "                    dbeta = np.diff(beta_series)\n",
    "                    smooth_pen = np.nanvar(dbeta) if len(dbeta)>1 else 0.0\n",
    "                    anchor_pen = np.nanmean((beta_series - beta_anchor)**2)\n",
    "                    obj = rmse_tail + w_anchor*anchor_pen + w_smooth*smooth_pen\n",
    "\n",
    "                    if (best is None) or (obj < best[\"obj\"]):\n",
    "                        best = {\"qa\": qa, \"qb\": qb, \"rr\": rr, \"obj\": obj}\n",
    "\n",
    "        # Refit em TRAIN+TEST com os hiperparâmetros escolhidos\n",
    "        x_block = pd.concat([x_tr, x_te], axis=0)\n",
    "        y_block = pd.concat([y_tr, y_te], axis=0)\n",
    "        kf_block = kalman_alpha_beta(\n",
    "            y_block, x_block,\n",
    "            q_alpha=best[\"qa\"], q_beta=best[\"qb\"], r=best[\"rr\"],\n",
    "            alpha0=alpha0, beta0=beta0, p0_alpha=p0_alpha, p0_beta=p0_beta\n",
    "        )\n",
    "        kk = kf_block.loc[te]\n",
    "        a_full.loc[te]  = kk[\"alpha_t\"]\n",
    "        b_full.loc[te]  = kk[\"beta_t\"]\n",
    "        sa_full.loc[te] = kk[\"sd_alpha_t\"]\n",
    "        sb_full.loc[te] = kk[\"sd_beta_t\"]\n",
    "        s_full.loc[te]  = kk[\"spread_kf\"]\n",
    "\n",
    "        i += test\n",
    "\n",
    "    return (a_full.rename(\"alpha_t\"),\n",
    "            b_full.rename(\"beta_t\"),\n",
    "            sa_full.rename(\"sd_alpha_t\"),\n",
    "            sb_full.rename(\"sd_beta_t\"),\n",
    "            s_full.rename(\"spread_kf\"))\n",
    "\n",
    "\n",
    "# ============================ KMM  ============================\n",
    "def hmm_gaussian_em(y: np.ndarray,\n",
    "                    n_states: int = 2,\n",
    "                    max_iter: int = 100,\n",
    "                    tol: float = 1e-4,\n",
    "                    p_stay_init: float = 0.95):\n",
    "    \"\"\"\n",
    "    EM (Baum–Welch) para HMM Gaussiano 1D.\n",
    "    Retorna dict com: pi (K,), A (K,K), mu (K,), var (K,), loglik.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, float)\n",
    "    T = len(y); K = int(n_states)\n",
    "    if T < K+2:  # proteção\n",
    "        raise ValueError(\"Série curta para HMM.\")\n",
    "\n",
    "    # Inicializações simples e robustas\n",
    "    pi = np.full(K, 1.0/K)\n",
    "    A  = np.full((K, K), (1 - p_stay_init) / (K - 1))\n",
    "    np.fill_diagonal(A, p_stay_init)\n",
    "\n",
    "    # médias em quantis, variâncias iguais\n",
    "    qs = np.linspace(0.0, 1.0, K+2)[1:-1]\n",
    "    mu = np.quantile(y, qs) if K > 1 else np.array([np.median(y)])\n",
    "    var = np.full(K, np.var(y) + 1e-6)\n",
    "\n",
    "    prev_ll = -np.inf\n",
    "    for _ in range(max_iter):\n",
    "        # ---- E-step: forward-backward em log ----\n",
    "        # log-emissões para todos os estados\n",
    "        logB = np.vstack([_norm_logpdf(y, mu[k], var[k]) for k in range(K)]).T  # (T,K)\n",
    "\n",
    "        # forward (log-alpha)\n",
    "        log_alpha = np.empty((T, K))\n",
    "        log_alpha[0] = np.log(pi) + logB[0]\n",
    "        for t in range(1, T):\n",
    "            log_alpha[t] = logB[t] + _logsumexp(log_alpha[t-1][:,None] + np.log(A), axis=0)\n",
    "\n",
    "        # log verossimilhança\n",
    "        ll = _logsumexp(log_alpha[-1], axis=0)\n",
    "\n",
    "        # backward (log-beta)\n",
    "        log_beta = np.zeros((T, K))\n",
    "        for t in range(T-2, -1, -1):\n",
    "            log_beta[t] = _logsumexp(np.log(A) + (logB[t+1] + log_beta[t+1])[None,:], axis=1)\n",
    "\n",
    "        # gammas/xis (em prob.)\n",
    "        log_gamma = log_alpha + log_beta - ll\n",
    "        gamma = np.exp(log_gamma)                           # (T,K)\n",
    "        xi = np.zeros((T-1, K, K))\n",
    "        for t in range(T-1):\n",
    "            z = (log_alpha[t][:,None] + np.log(A) + (logB[t+1] + log_beta[t+1])[None,:])\n",
    "            z -= _logsumexp(z, axis=None)  # norma total\n",
    "            xi[t] = np.exp(z)\n",
    "\n",
    "        # ---- M-step ----\n",
    "        pi = np.clip(gamma[0], 1e-12, None); pi /= pi.sum()\n",
    "        A = np.clip(xi.sum(axis=0), 1e-12, None)\n",
    "        A /= A.sum(axis=1, keepdims=True)\n",
    "\n",
    "        w = np.clip(gamma.sum(axis=0), 1e-12, None)        # (K,)\n",
    "        mu = (gamma * y[:,None]).sum(axis=0) / w\n",
    "        var = (gamma * ((y[:,None]-mu)**2)).sum(axis=0) / w\n",
    "        var = np.maximum(var, 1e-8)\n",
    "\n",
    "        # critério de parada\n",
    "        if ll - prev_ll < tol:\n",
    "            break\n",
    "        prev_ll = ll\n",
    "\n",
    "    return {\"pi\": pi, \"A\": A, \"mu\": mu, \"var\": var, \"loglik\": float(ll)}\n",
    "\n",
    "def hmm_filter_proba(y: np.ndarray, params: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Probabilidade filtrada p(s_t=k | y_1:t) (sem look-ahead).\n",
    "    Retorna (T,K). Usa somente forward normalizado.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, float)\n",
    "    pi, A, mu, var = params[\"pi\"], params[\"A\"], params[\"mu\"], params[\"var\"]\n",
    "    T = len(y); K = len(pi)\n",
    "\n",
    "    logB = np.vstack([_norm_logpdf(y, mu[k], var[k]) for k in range(K)]).T\n",
    "    log_alpha = np.empty((T, K))\n",
    "    log_alpha[0] = np.log(pi) + logB[0]\n",
    "    for t in range(1, T):\n",
    "        log_alpha[t] = logB[t] + _logsumexp(log_alpha[t-1][:,None] + np.log(A), axis=0)\n",
    "\n",
    "    # normaliza em cada t -> probas filtradas\n",
    "    log_alpha -= _logsumexp(log_alpha, axis=1)[:,None]\n",
    "    return np.exp(log_alpha)  # (T,K)\n",
    "\n",
    "def fit_hmm_walkforward(obs: pd.Series,\n",
    "                        train: int = 252,\n",
    "                        test: int = 63,\n",
    "                        n_states: int = 2,\n",
    "                        max_iter: int = 100,\n",
    "                        tol: float = 1e-4,\n",
    "                        p_stay_init: float = 0.95):\n",
    "    \"\"\"\n",
    "    Treina HMM em janelas TRAIN e devolve p(MR) filtrada (apenas no TEST).\n",
    "    Estado 'MR' = argmin_k [ var_k + |mu_k| ]  (baixa variância e pouco viés).\n",
    "    \"\"\"\n",
    "    obs = pd.Series(obs).astype(float).dropna()\n",
    "    idx = obs.index\n",
    "    p_mr_full = pd.Series(index=idx, dtype=float)\n",
    "\n",
    "    i = train\n",
    "    while i + test <= len(idx):\n",
    "        tr = slice(idx[i-train], idx[i-1])\n",
    "        te = slice(idx[i], idx[i+test-1])\n",
    "        y_tr = obs.loc[tr].values\n",
    "        y_block = obs.loc[tr].append(obs.loc[te]).values  # train+test\n",
    "\n",
    "        try:\n",
    "            pars = hmm_gaussian_em(y_tr, n_states=n_states,\n",
    "                                   max_iter=max_iter, tol=tol,\n",
    "                                   p_stay_init=p_stay_init)\n",
    "        except Exception:\n",
    "            i += test\n",
    "            continue\n",
    "\n",
    "        # estado MR pelo score var+|mu|\n",
    "        score = pars[\"var\"] + np.abs(pars[\"mu\"])\n",
    "        k_mr = int(np.argmin(score))\n",
    "\n",
    "        # probabilidades filtradas no bloco todo e coleta do TEST\n",
    "        P = hmm_filter_proba(y_block, pars)  # (T_block,K)\n",
    "        Ttr = len(y_tr)\n",
    "        p_mr = P[Ttr:, k_mr]                 # somente TEST\n",
    "        p_mr_full.loc[te] = p_mr\n",
    "\n",
    "        i += test\n",
    "\n",
    "    return p_mr_full.rename(\"p_mr\")\n",
    "\n",
    "\n",
    "def _logsumexp(a: np.ndarray, axis=None):\n",
    "    a_max = np.max(a, axis=axis, keepdims=True)\n",
    "    out = a_max + np.log(np.sum(np.exp(a - a_max), axis=axis, keepdims=True))\n",
    "    return np.squeeze(out, axis=axis)\n",
    "\n",
    "def _norm_logpdf(y: np.ndarray, mu: np.ndarray, var: np.ndarray, eps: float = 1e-12):\n",
    "    var = np.maximum(var, eps)\n",
    "    return -0.5 * (np.log(2*np.pi) + np.log(var) + ((y - mu)**2)/var)\n",
    "\n",
    "# ============================ MÉTRICAS/DIAGNÓSTICO ============================\n",
    "\n",
    "def estimate_bars_per_day_from_index(index: pd.DatetimeIndex) -> int:\n",
    "    if len(index) == 0: return 1\n",
    "    counts = pd.Series(index.normalize()).value_counts()\n",
    "    return int(np.median(counts.values))\n",
    "\n",
    "def diagnostics_quick(df_pair: pd.DataFrame):\n",
    "    x = np.log(df_pair['close_bdr'])\n",
    "    y = np.log(df_pair['close_us'] * df_pair['usdxbrl'])\n",
    "    valid = x.notna() & y.notna()\n",
    "    corr = x[valid].corr(y[valid]) if valid.any() else np.nan\n",
    "    ratio_inst = (df_pair['close_bdr'] / (df_pair['close_us'] * df_pair['usdxbrl'])).replace([np.inf,-np.inf], np.nan)\n",
    "    med_ratio = float(ratio_inst.median())\n",
    "    return {\"corr_log\": float(corr), \"median_ratio\": med_ratio, \"n\": int(valid.sum())}\n",
    "\n",
    "# ============================ BACKTEST (next-bar, custos) ============================\n",
    "\n",
    "def backtest_next_bar(df: pd.DataFrame,\n",
    "                      z_col=\"z\",\n",
    "                      theo_col=\"bdr_teo\",\n",
    "                      z_entry=2.0, z_exit=0.75, z_stop=3.0,\n",
    "                      slippage_bps=0.0, commission_bps=0.0,\n",
    "                      borrow_bps_pa=400.0, fx_basis_bps=20.0,\n",
    "                      use_risk_sizing=True, ewm_alpha=0.06,\n",
    "                      target_vol=0.08, max_leverage=2.0,\n",
    "                      gate: pd.Series | None = None,\n",
    "                      sd_beta: pd.Series | None = None,\n",
    "                      x_log: pd.Series | None = None,\n",
    "                      uncertainty_weight: float = 1.0):\n",
    "    \"\"\"\n",
    "    Backtest do spread S = BDR - Teórico (preço), execução NEXT-BAR.\n",
    "    Sizing por risco usa EWMA(std(S)) e, opcionalmente, adiciona incerteza de β (lagged):\n",
    "\n",
    "        var_eff_t = var_EWMA(S)_t  +  (theo_{t-1} * ln(X_t))^2 * Var(β_{t-1})\n",
    "\n",
    "    onde X_t = teórico estático (ADR×FX×ρ) em preço; Var(β) ≈ sd_beta^2.\n",
    "\n",
    "    Custos:\n",
    "      - slippage + comissão por turnover |Δpos|*(pxB+pxT) * (bps/1e4)\n",
    "      - aluguel short BDR (sobre notional da perna BDR)\n",
    "      - FX basis (sobre notional da perna sintética)\n",
    "    \"\"\"\n",
    "    df = df.copy().sort_index()\n",
    "\n",
    "    S   = (df[\"close_bdr\"] - df[theo_col]).rename(\"S\")\n",
    "    z   = df[z_col].copy()\n",
    "    pxB = df[\"close_bdr\"]\n",
    "    pxT = df[theo_col]\n",
    "\n",
    "    # Gate\n",
    "    if gate is not None:\n",
    "        gate = gate.reindex(df.index).fillna(False)\n",
    "    else:\n",
    "        gate = pd.Series(True, index=df.index)\n",
    "\n",
    "    # var efetiva (EWMA do spread + incerteza beta)\n",
    "    if use_risk_sizing:\n",
    "        base_var = (S - S.ewm(alpha=ewm_alpha, adjust=False).mean()).pow(2).ewm(alpha=ewm_alpha, adjust=False).mean()\n",
    "        extra = 0.0\n",
    "        if (sd_beta is not None) and (x_log is not None):\n",
    "            sd_b_lag = sd_beta.reindex(df.index).shift(1).fillna(method=\"ffill\").fillna(0.0)\n",
    "            lnX = x_log.reindex(df.index).fillna(method=\"ffill\").fillna(0.0)  # log do teórico estático\n",
    "            theo_lag = pxT.shift(1).fillna(method=\"ffill\").fillna(pxT.iloc[0])\n",
    "            extra = (theo_lag * lnX).pow(2) * (sd_b_lag.pow(2)) * float(uncertainty_weight)\n",
    "        var_eff = (base_var + extra).replace(0, np.nan)\n",
    "        vol_eff = np.sqrt(var_eff).shift(1)\n",
    "        w = (target_vol / vol_eff).clip(upper=max_leverage).fillna(0.0)\n",
    "    else:\n",
    "        w = pd.Series(target_vol, index=df.index).clip(upper=max_leverage)\n",
    "\n",
    "    # estado discreto\n",
    "    state = pd.Series(0, index=df.index, dtype=int)\n",
    "    pos = 0\n",
    "    for t, zz in z.items():\n",
    "        if not gate.loc[t]:\n",
    "            pos = 0\n",
    "        else:\n",
    "            if pos == 0:\n",
    "                if zz >  z_entry: pos = -1\n",
    "                if zz < -z_entry: pos = +1\n",
    "            else:\n",
    "                if abs(zz) > z_stop:\n",
    "                    pos = 0\n",
    "                elif abs(zz) < z_exit:\n",
    "                    pos = 0\n",
    "        state.loc[t] = pos\n",
    "\n",
    "    # posição efetiva (contínua)\n",
    "    pos_w = (state * w).rename(\"pos\")\n",
    "\n",
    "    # Execução next-bar\n",
    "    pos_exec = pos_w.shift(1).fillna(0.0)\n",
    "    dS = S.diff().fillna(0.0)\n",
    "    pnl_gross = (pos_exec * dS).rename(\"pnl_gross\")\n",
    "\n",
    "    # Turnover e custos de slippage/comissão (por mudança de posição)\n",
    "    dpos = pos_exec.diff().abs().fillna(pos_exec.abs())\n",
    "    bps_trading = (slippage_bps + commission_bps) / 1e4\n",
    "    trade_cost = (dpos * (pxB.shift(1).fillna(pxB) + pxT.shift(1).fillna(pxT)) * bps_trading).rename(\"trade_cost\")\n",
    "\n",
    "    # Custos recorrentes (diários aprox)\n",
    "    days = (df.index.to_series().diff().dt.days.fillna(0)).clip(lower=0)\n",
    "    borrow_daily = (borrow_bps_pa/1e4) / 252.0\n",
    "    fx_basis_daily = (fx_basis_bps/1e4) / 252.0\n",
    "\n",
    "    borrow_cost = (pos_exec.clip(upper=0).abs() * pxB.shift(1) * borrow_daily * (days.replace(0,1))).rename(\"borrow_cost\")\n",
    "    fx_cost     = (pos_exec.abs() * pxT.shift(1) * fx_basis_daily * (days.replace(0,1))).rename(\"fx_cost\")\n",
    "\n",
    "    pnl_net = (pnl_gross - trade_cost - borrow_cost - fx_cost).rename(\"pnl_net\")\n",
    "    equity  = pnl_net.cumsum().rename(\"equity\")\n",
    "\n",
    "    # métricas\n",
    "    bars_per_day = estimate_bars_per_day_from_index(df.index)\n",
    "    ann_factor = np.sqrt(max(bars_per_day,1) * 252.0)\n",
    "    ret = pnl_net\n",
    "    std = float(ret.std(ddof=1))\n",
    "    sharpe = (float(ret.mean()) / (std + 1e-12)) * ann_factor if std > 0 else np.nan\n",
    "    roll_max = equity.cummax()\n",
    "    mdd = float((equity - roll_max).min()) if len(equity) else np.nan\n",
    "\n",
    "    # estatística de trades\n",
    "    turns = (state.shift(1).fillna(0) != state).astype(int)\n",
    "    trade_idx = turns[turns==1].index\n",
    "    trades = int((state.abs().diff().fillna(state.abs()) > 0).sum())\n",
    "    # winrate/expectancy aproximados por janelas entre flips\n",
    "    wins = 0; losses = 0; pnl_trades = []\n",
    "    if len(trade_idx) > 1:\n",
    "        starts = trade_idx\n",
    "        stops = list(starts[1:]) + [df.index[-1]]\n",
    "        for s, e in zip(starts, stops):\n",
    "            seg = equity.loc[s:e]\n",
    "            if len(seg) >= 2:\n",
    "                dp = float(seg.iloc[-1] - seg.iloc[0])\n",
    "                pnl_trades.append(dp)\n",
    "                if dp > 0: wins += 1\n",
    "                elif dp < 0: losses += 1\n",
    "    winrate = (wins / max(wins+losses,1)) if (wins+losses)>0 else np.nan\n",
    "    expectancy = float(np.nanmedian(pnl_trades)) if len(pnl_trades)>0 else np.nan\n",
    "    # holding mediano em barras\n",
    "    hold_lengths = []\n",
    "    if len(trade_idx) > 1:\n",
    "        for s, e in zip(trade_idx, list(trade_idx[1:]) + [df.index[-1]]):\n",
    "            hold_lengths.append(max(1, (df.index.get_loc(e) - df.index.get_loc(s))))\n",
    "    med_hold = int(np.median(hold_lengths)) if hold_lengths else 0\n",
    "\n",
    "    out = pd.concat([S, z, pos_w, pnl_gross, trade_cost, borrow_cost, fx_cost, pnl_net, equity], axis=1)\n",
    "    summary = {\n",
    "        \"bars\": len(df),\n",
    "        \"trades\": trades,\n",
    "        \"winrate\": winrate,\n",
    "        \"expectancy_per_trade\": expectancy,\n",
    "        \"median_hold_bars\": med_hold,\n",
    "        \"sharpe_hourly_annualized\": sharpe,\n",
    "        \"mdd\": mdd,\n",
    "        \"pnl_total\": float(equity.iloc[-1]) if len(equity) else 0.0,\n",
    "    }\n",
    "    return out, summary\n",
    "\n",
    "# ============================ PLOTS (opcional) ============================\n",
    "\n",
    "def plot_prices(df: pd.DataFrame, name: str, outdir: Path):\n",
    "    fig = plt.figure()\n",
    "    df[['close_bdr','bdr_teo']].dropna().plot(ax=plt.gca())\n",
    "    plt.title(f\"{name} — BDR vs Teórico\")\n",
    "    plt.xlabel(\"Tempo\"); plt.ylabel(\"Preço (BRL)\")\n",
    "    fig.tight_layout(); fig.savefig(outdir / f\"{name}_prices.png\"); plt.close(fig)\n",
    "\n",
    "def plot_spread(df: pd.DataFrame, name: str, outdir: Path):\n",
    "    fig = plt.figure()\n",
    "    (df['close_bdr'] - df['bdr_teo']).dropna().plot(ax=plt.gca())\n",
    "    plt.title(f\"{name} — Spread (BDR − Teórico)\")\n",
    "    plt.xlabel(\"Tempo\"); plt.ylabel(\"Spread (BRL)\")\n",
    "    fig.tight_layout(); fig.savefig(outdir / f\"{name}_spread.png\"); plt.close(fig)\n",
    "\n",
    "def plot_zscore(df: pd.DataFrame, name: str, outdir: Path):\n",
    "    fig = plt.figure()\n",
    "    df['z'].dropna().plot(ax=plt.gca())\n",
    "    plt.title(f\"{name} — Z-Score do Spread (robusto, shift(1))\")\n",
    "    plt.xlabel(\"Tempo\"); plt.ylabel(\"Z\")\n",
    "    ax = plt.gca(); ax.axhline(0); ax.axhline(2); ax.axhline(-2)\n",
    "    fig.tight_layout(); fig.savefig(outdir / f\"{name}_zscore.png\"); plt.close(fig)\n",
    "\n",
    "# ============================ PIPELINE ============================\n",
    "\n",
    "def run_pipeline_one_pair(bdr: str, adr: str,\n",
    "                          bdr_map: dict, us_map: dict, fx_df: pd.DataFrame,\n",
    "                          out_pairs_dir: Path, plots_dir: Path):\n",
    "    # 1) Shift do BDR\n",
    "    df_bdr = shift_bdr_index_with_eod_rule(\n",
    "        bdr_map[bdr],\n",
    "        base_min=CFG[\"shift_bdr_base_min\"],\n",
    "        eod_min=CFG[\"shift_bdr_eod_min\"],\n",
    "    )\n",
    "\n",
    "    # 2) Nearest join BDR↔ADR↔FX (backward)\n",
    "    tmp = nearest_join(\n",
    "        df_bdr[['close_bdr','volume_bdr']],\n",
    "        us_map[adr][['close_us','volume_us']],\n",
    "        CFG[\"asof_tolerance\"]\n",
    "    )\n",
    "    tmp = nearest_join(tmp, fx_df[['usdxbrl','volume_fx']], CFG[\"asof_tolerance\"])\n",
    "    tmp = tmp.dropna(subset=['close_bdr','close_us','usdxbrl']).sort_index()\n",
    "\n",
    "    if len(tmp) < CFG[\"min_overlap\"]:\n",
    "        log.info(f\"[{bdr}↔{adr}] overlap insuficiente: {len(tmp)}\")\n",
    "        return None\n",
    "\n",
    "    # 3) ρ (ratio) usando apenas o primeiro TRAIN (evita look-ahead)\n",
    "    n_train = min(CFG[\"wfa_train\"], len(tmp)//3 or 1)\n",
    "    tmp_head = tmp.iloc[:n_train]\n",
    "    ratio_ols, ratio_sd_raw, n_ratio_raw = calibrate_ratio(\n",
    "        tmp_head['close_bdr'], tmp_head['close_us'], tmp_head['usdxbrl'], method=\"ols\"\n",
    "    )\n",
    "    ratio_used, ratio_sd, n_ratio, ratio_method = choose_ratio_with_fallback(tmp_head, ratio_ols)\n",
    "    tmp[\"bdr_teo\"] = theoretical_bdr_brl(tmp[\"close_us\"], tmp[\"usdxbrl\"], ratio_used)\n",
    "\n",
    "    # 4) KF Walk-Forward (α_t, β_t) sobre LOGs (com validação interna e penalizações)\n",
    "    x_log = np.log(tmp[\"bdr_teo\"])\n",
    "    y_log = np.log(tmp[\"close_bdr\"])\n",
    "\n",
    "    alpha_t, beta_t, sd_alpha_t, sd_beta_t, spread_kf_log = fit_kf_walkforward(\n",
    "        x_log, y_log,\n",
    "        q_alpha_grid=CFG[\"q_alpha_grid\"],\n",
    "        q_beta_grid=CFG[\"q_beta_grid\"],\n",
    "        r_grid=CFG[\"r_grid\"],\n",
    "        train=CFG[\"wfa_train\"],\n",
    "        test=CFG[\"wfa_test\"],\n",
    "        alpha0=CFG[\"init_alpha\"], beta0=CFG[\"init_beta\"],\n",
    "        p0_alpha=CFG[\"init_p_alpha\"], p0_beta=CFG[\"init_p_beta\"],\n",
    "        w_anchor=CFG[\"kf_w_anchor\"], beta_anchor=CFG[\"kf_beta_anchor\"], w_smooth=CFG[\"kf_w_smooth\"]\n",
    "    )\n",
    "\n",
    "    tmp[\"alpha_t\"]   = alpha_t\n",
    "    tmp[\"beta_t\"]    = beta_t\n",
    "    tmp[\"sd_beta_t\"] = sd_beta_t\n",
    "    # (lag para trading)\n",
    "    tmp[\"alpha_lag\"] = tmp[\"alpha_t\"].shift(1)\n",
    "    tmp[\"beta_lag\"]  = tmp[\"beta_t\"].shift(1)\n",
    "\n",
    "    # clip de β (lagged) para estabilidade\n",
    "    beta_lo, beta_hi = CFG[\"kf_beta_clip\"]\n",
    "    beta_lag_clipped = tmp[\"beta_lag\"].clip(lower=beta_lo, upper=beta_hi)\n",
    "\n",
    "    # teórico dinâmico (preço): exp(α) * X^β  com α/β defasados\n",
    "    tmp[\"bdr_teo_kf\"] = np.exp(tmp[\"alpha_lag\"]) * (tmp[\"bdr_teo\"] ** beta_lag_clipped)\n",
    "\n",
    "    # 5) Spreads e Z-scores\n",
    "    spread_base = (tmp[\"close_bdr\"] - tmp[\"bdr_teo\"]).rename(\"spread_base\")\n",
    "    spread_kf   = (tmp[\"close_bdr\"] - tmp[\"bdr_teo_kf\"]).rename(\"spread_kf_price\")\n",
    "    tmp[\"z\"]    = robust_zscore(spread_base, ewm_alpha=CFG[\"ewm_alpha\"], winsor_pct=CFG[\"winsor_pct\"])\n",
    "    tmp[\"z_kf\"] = robust_zscore(spread_kf,   ewm_alpha=CFG[\"ewm_alpha\"], winsor_pct=CFG[\"winsor_pct\"])\n",
    "\n",
    "    # 6) Gate de cointegração (rolling) em LOGs + gate de estabilidade do KF\n",
    "    tmp[\"coint_on\"] = rolling_coint_gate(x_log, y_log, L=CFG[\"coint_L\"], pval_th=CFG[\"coint_pval\"])\n",
    "    if CFG[\"use_kf_stability_gate\"]:\n",
    "        sd_ok = (tmp[\"sd_beta_t\"].shift(1) <= CFG[\"kf_sd_beta_max\"])\n",
    "        beta_ok = (beta_lag_clipped == tmp[\"beta_lag\"].shift(1).clip(beta_lo, beta_hi))  # True mesmo após clip\n",
    "        tmp[\"gate_kf\"] = (tmp[\"coint_on\"] & sd_ok & beta_ok).fillna(False)\n",
    "    else:\n",
    "        tmp[\"gate_kf\"] = tmp[\"coint_on\"]\n",
    "\n",
    "    # === HMM regime gate (opcional) ===\n",
    "    if CFG.get(\"use_hmm_gate\", False):\n",
    "        # escolhe observável para o HMM\n",
    "        obs_map = {\n",
    "            \"dspread_kf\": spread_kf.diff().fillna(0.0),\n",
    "            \"spread_kf\":  spread_kf,\n",
    "            \"z_kf\":       tmp[\"z_kf\"],\n",
    "            \"spread\":     spread_base,\n",
    "            \"z\":          tmp[\"z\"],\n",
    "        }\n",
    "        obs_hmm = obs_map.get(CFG[\"hmm_obs\"], spread_kf.diff().fillna(0.0)).dropna()\n",
    "\n",
    "        p_mr = fit_hmm_walkforward(\n",
    "            obs_hmm,\n",
    "            train=CFG[\"wfa_train\"],\n",
    "            test=CFG[\"wfa_test\"],\n",
    "            n_states=CFG[\"hmm_n_states\"],\n",
    "            max_iter=CFG[\"hmm_max_iter\"],\n",
    "            tol=CFG[\"hmm_tol\"],\n",
    "            p_stay_init=CFG[\"hmm_p_stay_init\"]\n",
    "        )\n",
    "        # alinha e defasa (evita look-ahead)\n",
    "        tmp[\"p_mr\"] = p_mr.reindex(tmp.index)\n",
    "        tmp[\"gate_hmm\"] = (tmp[\"p_mr\"].shift(1) >= CFG[\"hmm_p_threshold\"]).fillna(False)\n",
    "\n",
    "        # combina com gate do KF\n",
    "        tmp[\"gate_kf_hmm\"] = (tmp[\"gate_kf\"] & tmp[\"gate_hmm\"]).fillna(False)\n",
    "\n",
    "    # 7) Diagnósticos\n",
    "    mae_base  = (tmp[\"close_bdr\"] - tmp[\"bdr_teo\"]).abs().mean()\n",
    "    mae_kf    = (tmp[\"close_bdr\"] - tmp[\"bdr_teo_kf\"]).abs().mean()\n",
    "    diag = diagnostics_quick(tmp)\n",
    "\n",
    "    # 8) Salvar parquet do par\n",
    "    out_path = out_pairs_dir / f\"{bdr}_{adr}.parquet\"\n",
    "    tmp.reset_index().rename(columns={'index':'datetime'}).to_parquet(out_path, index=False)\n",
    "\n",
    "    # 9) Plots (baseline)\n",
    "    plot_prices(tmp, f\"{bdr}_{adr}\", plots_dir)\n",
    "    plot_spread(tmp, f\"{bdr}_{adr}\", plots_dir)\n",
    "    plot_zscore(tmp.rename(columns={\"z\":\"z\"}), f\"{bdr}_{adr}\", plots_dir)\n",
    "\n",
    "    # Baseline\n",
    "    bt_base, sum_base = backtest_next_bar(\n",
    "        tmp,\n",
    "        z_col=\"z\",\n",
    "        theo_col=\"bdr_teo\",\n",
    "        z_entry=CFG[\"z_enter\"], z_exit=CFG[\"z_exit\"], z_stop=CFG[\"z_stop\"],\n",
    "        slippage_bps=CFG[\"slippage_bps\"], commission_bps=CFG[\"commission_bps\"],\n",
    "        borrow_bps_pa=CFG[\"borrow_bps_pa\"], fx_basis_bps=CFG[\"fx_basis_bps\"],\n",
    "        use_risk_sizing=True, ewm_alpha=CFG[\"ewm_alpha\"],\n",
    "        target_vol=CFG[\"target_vol\"], max_leverage=CFG[\"max_leverage\"],\n",
    "        gate=tmp[\"coint_on\"],\n",
    "        sd_beta=None, x_log=x_log,\n",
    "        uncertainty_weight=CFG[\"uncertainty_weight\"]\n",
    "    )\n",
    "\n",
    "    # Kalman (dinâmico)\n",
    "    bt_kf, sum_kf = backtest_next_bar(\n",
    "        tmp,\n",
    "        z_col=\"z_kf\",\n",
    "        theo_col=\"bdr_teo_kf\",\n",
    "        z_entry=CFG[\"z_enter\"], z_exit=CFG[\"z_exit\"], z_stop=CFG[\"z_stop\"],\n",
    "        slippage_bps=CFG[\"slippage_bps\"], commission_bps=CFG[\"commission_bps\"],\n",
    "        borrow_bps_pa=CFG[\"borrow_bps_pa\"], fx_basis_bps=CFG[\"fx_basis_bps\"],\n",
    "        use_risk_sizing=True, ewm_alpha=CFG[\"ewm_alpha\"],\n",
    "        target_vol=CFG[\"target_vol\"], max_leverage=CFG[\"max_leverage\"],\n",
    "        gate=tmp[\"gate_kf\"],\n",
    "        sd_beta=tmp[\"sd_beta_t\"], x_log=x_log,\n",
    "        uncertainty_weight=CFG[\"uncertainty_weight\"]\n",
    "    )\n",
    "\n",
    "    # Kalman + HMM (se ligado)\n",
    "    sum_kf_hmm = None\n",
    "    if CFG.get(\"use_hmm_gate\", False):\n",
    "        bt_kf_hmm, sum_kf_hmm = backtest_next_bar(\n",
    "            tmp,\n",
    "            z_col=\"z_kf\",\n",
    "            theo_col=\"bdr_teo_kf\",\n",
    "            z_entry=CFG[\"z_enter\"], z_exit=CFG[\"z_exit\"], z_stop=CFG[\"z_stop\"],\n",
    "            slippage_bps=CFG[\"slippage_bps\"], commission_bps=CFG[\"commission_bps\"],\n",
    "            borrow_bps_pa=CFG[\"borrow_bps_pa\"], fx_basis_bps=CFG[\"fx_basis_bps\"],\n",
    "            use_risk_sizing=True, ewm_alpha=CFG[\"ewm_alpha\"],\n",
    "            target_vol=CFG[\"target_vol\"], max_leverage=CFG[\"max_leverage\"],\n",
    "            gate=tmp[\"gate_kf_hmm\"],\n",
    "            sd_beta=tmp[\"sd_beta_t\"], x_log=x_log,\n",
    "            uncertainty_weight=CFG[\"uncertainty_weight\"]\n",
    "        )\n",
    "        bt_kf_hmm_path = out_pairs_dir / f\"{bdr}_{adr}_bt_kf_hmm.parquet\"\n",
    "        bt_kf_hmm.reset_index().rename(columns={'index':'datetime'}).to_parquet(bt_kf_hmm_path, index=False)\n",
    "\n",
    "    # salvar curvas base e KF\n",
    "    bt_base_path = out_pairs_dir / f\"{bdr}_{adr}_bt_base.parquet\"\n",
    "    bt_kf_path   = out_pairs_dir / f\"{bdr}_{adr}_bt_kf.parquet\"\n",
    "    bt_base.reset_index().rename(columns={'index':'datetime'}).to_parquet(bt_base_path, index=False)\n",
    "    bt_kf.reset_index().rename(columns={'index':'datetime'}).to_parquet(bt_kf_path,   index=False)\n",
    "\n",
    "    # resumo\n",
    "    summary = {\n",
    "        \"bdr\": bdr, \"adr\": adr,\n",
    "        \"overlap_obs\": int(len(tmp)),\n",
    "        \"ratio_used\": float(ratio_used),\n",
    "        \"ratio_method\": ratio_method,\n",
    "        \"mae_teorico\": float(mae_base),\n",
    "        \"mae_kf\": float(mae_kf),\n",
    "        \"corr_log\": float(diag[\"corr_log\"]),\n",
    "        \"median_ratio_diag\": float(diag[\"median_ratio\"]),\n",
    "        # backtests\n",
    "        \"sharpe_base\": sum_base[\"sharpe_hourly_annualized\"],\n",
    "        \"pnl_base\": sum_base[\"pnl_total\"],\n",
    "        \"mdd_base\": sum_base[\"mdd\"],\n",
    "        \"trades_base\": sum_base[\"trades\"],\n",
    "        \"sharpe_kf\": sum_kf[\"sharpe_hourly_annualized\"],\n",
    "        \"pnl_kf\": sum_kf[\"pnl_total\"],\n",
    "        \"mdd_kf\": sum_kf[\"mdd\"],\n",
    "        \"trades_kf\": sum_kf[\"trades\"],\n",
    "    }\n",
    "    if CFG.get(\"use_hmm_gate\", False) and (sum_kf_hmm is not None):\n",
    "        summary.update({\n",
    "            \"sharpe_kf_hmm\": sum_kf_hmm[\"sharpe_hourly_annualized\"],\n",
    "            \"pnl_kf_hmm\":    sum_kf_hmm[\"pnl_total\"],\n",
    "            \"mdd_kf_hmm\":    sum_kf_hmm[\"mdd\"],\n",
    "            \"trades_kf_hmm\": sum_kf_hmm[\"trades\"],\n",
    "        })\n",
    "\n",
    "    # logging\n",
    "    base_msg = (f\"[{bdr}_{adr}] BASE Sharpe={summary['sharpe_base']:.2f} \"\n",
    "                f\"({summary['trades_base']} trades) | \"\n",
    "                f\"KF Sharpe={summary['sharpe_kf']:.2f} \"\n",
    "                f\"({summary['trades_kf']} trades) | overlap={summary['overlap_obs']}\")\n",
    "    if CFG.get(\"use_hmm_gate\", False) and (\"sharpe_kf_hmm\" in summary):\n",
    "        base_msg += f\" | KF+HMM Sharpe={summary['sharpe_kf_hmm']:.2f} ({summary['trades_kf_hmm']} trades)\"\n",
    "    log.info(base_msg)\n",
    "\n",
    "    return summary, out_path\n",
    "\n",
    "\n",
    "# ============================ REPORTS / GRÁFICOS ============================\n",
    "\n",
    "import os\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _bars_per_day(index: pd.DatetimeIndex) -> float:\n",
    "    if len(index) == 0: return 1.0\n",
    "    counts = pd.Series(index.normalize()).value_counts()\n",
    "    return float(np.median(counts.values))\n",
    "\n",
    "def _rolling_sharpe(pnl: pd.Series, win: int = 63, index: pd.DatetimeIndex | None = None) -> pd.Series:\n",
    "    \"\"\"Sharpe rolling (janela 'win'), anualizado pela cadência média (barras/dia).\"\"\"\n",
    "    pnl = pd.Series(pnl).dropna()\n",
    "    if index is None: index = pnl.index\n",
    "    ann = np.sqrt(max(_bars_per_day(index), 1.0) * 252.0)\n",
    "    mu = pnl.rolling(win).mean()\n",
    "    sd = pnl.rolling(win).std(ddof=1)\n",
    "    rs = (mu / (sd + 1e-12)) * ann\n",
    "    return rs.rename(f\"roll_sharpe_{win}\")\n",
    "\n",
    "def _empirical_transition_matrix(mr_flag: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Matriz 2x2 empírica a partir do flag de regime (0/1).\"\"\"\n",
    "    s = mr_flag.fillna(False).astype(int)\n",
    "    s_prev = s.shift(1)\n",
    "    mask = s_prev.notna()\n",
    "    s = s[mask]; s_prev = s_prev[mask].astype(int)\n",
    "    cm = np.zeros((2,2), dtype=float)\n",
    "    for i in (0,1):\n",
    "        for j in (0,1):\n",
    "            cm[i,j] = float(((s_prev==i) & (s==j)).sum())\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums==0] = 1.0\n",
    "    return cm / row_sums\n",
    "\n",
    "def _trade_segments_from_pos(pos: pd.Series):\n",
    "    \"\"\"Decompõe série de posição em trades (in/out) retornando lista de (t_in, t_out).\"\"\"\n",
    "    pos = pd.Series(pos).fillna(0.0)\n",
    "    on = (pos != 0.0).astype(int)\n",
    "    flips = on.diff().fillna(on).ne(0)\n",
    "    idx_flips = list(on[flips].index)\n",
    "    if not idx_flips:\n",
    "        return []\n",
    "    # garante que começamos em entrada\n",
    "    if on.loc[idx_flips[0]] == 0 and len(idx_flips) > 1:\n",
    "        idx_flips = idx_flips[1:]\n",
    "    trades = []\n",
    "    for k in range(0, len(idx_flips), 2):\n",
    "        t_in = idx_flips[k]\n",
    "        if k+1 < len(idx_flips):\n",
    "            t_out = idx_flips[k+1]\n",
    "        else:\n",
    "            t_out = pos.index[-1]\n",
    "        # trade válido tem pelo menos 1 passo\n",
    "        if t_in < t_out:\n",
    "            trades.append((t_in, t_out))\n",
    "    return trades\n",
    "\n",
    "def _mark_entries_exits_stops(pos: pd.Series, z: pd.Series, z_stop: float):\n",
    "    \"\"\"Detecta entradas/saídas e classifica saídas por stop (|z|>=z_stop) vs 'exit'.\"\"\"\n",
    "    pos = pd.Series(pos).fillna(0.0)\n",
    "    z = pd.Series(z)\n",
    "    state = (pos != 0.0).astype(int)\n",
    "    d = state.diff().fillna(state)\n",
    "    entries = list(state[d==+1].index)\n",
    "    exits   = list(state[d==-1].index)\n",
    "    stops, normal_exits = [], []\n",
    "    for t in exits:\n",
    "        if t in z.index and (abs(float(z.loc[t])) >= float(z_stop)):\n",
    "            stops.append(t)\n",
    "        else:\n",
    "            normal_exits.append(t)\n",
    "    return entries, normal_exits, stops\n",
    "\n",
    "def _save_df_to_excel(writer, sheet_name: str, df: pd.DataFrame):\n",
    "    df_out = df.copy()\n",
    "    if 'datetime' not in df_out.columns and isinstance(df_out.index, pd.DatetimeIndex):\n",
    "        df_out = df_out.reset_index().rename(columns={'index':'datetime'})\n",
    "    df_out.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def make_pair_report(pair_parquet_path: Path, out_root: Path, bdr: str, adr: str, cfg: dict):\n",
    "    \"\"\"\n",
    "    Lê os parquets gerados no pipeline e produz:\n",
    "      - BDR vs Teórico (estático & KF) + beta_t / sd_beta_t (2º eixo)\n",
    "      - Spread_kf & Z_kf com faixas de regime e marcações de trades\n",
    "      - P(MR) do HMM + threshold (se existir p_mr)\n",
    "      - Matriz de transição empírica (heatmap)\n",
    "      - Equities comparadas (BASE, KF, KF+HMM) + drawdown\n",
    "      - Distribuição de PnL por trade (estratificada)\n",
    "      - Turnover & custos (KF vs KF+HMM)\n",
    "      - Rolling Sharpe (63)\n",
    "      - Sensibilidade do threshold do HMM (linha)\n",
    "    Salva:\n",
    "      - PNGs em ../data/output/reports/{BDR}_{ADR}/\n",
    "      - Um Excel por par com uma aba por gráfico contendo *exatamente* os dados usados.\n",
    "    \"\"\"\n",
    "    name = f\"{bdr}_{adr}\"\n",
    "    report_dir = out_root / \"reports\" / name\n",
    "    _ensure_dir(report_dir)\n",
    "\n",
    "    # === Carrega o .parquet do par e os backtests ===\n",
    "    df = pd.read_parquet(pair_parquet_path).set_index('datetime')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    pairs_dir = pair_parquet_path.parent\n",
    "    bt_base_path = pairs_dir / f\"{name}_bt_base.parquet\"\n",
    "    bt_kf_path   = pairs_dir / f\"{name}_bt_kf.parquet\"\n",
    "    bt_kf_hmm_path = pairs_dir / f\"{name}_bt_kf_hmm.parquet\"\n",
    "\n",
    "    bt_base = pd.read_parquet(bt_base_path).set_index('datetime') if bt_base_path.exists() else None\n",
    "    bt_kf   = pd.read_parquet(bt_kf_path).set_index('datetime')   if bt_kf_path.exists()   else None\n",
    "    bt_hmm  = pd.read_parquet(bt_kf_hmm_path).set_index('datetime') if bt_kf_hmm_path.exists() else None\n",
    "\n",
    "    # Excel writer\n",
    "    xlsx_path = report_dir / f\"{name}_data_for_plots.xlsx\"\n",
    "    with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\") as writer:\n",
    "\n",
    "        # ---------------- (1) BDR vs Teórico (estático & KF) ----------------\n",
    "        d1 = pd.DataFrame({\n",
    "            \"close_bdr\":  df[\"close_bdr\"],\n",
    "            \"bdr_teo\":    df[\"bdr_teo\"],\n",
    "            \"bdr_teo_kf\": df.get(\"bdr_teo_kf\", pd.Series(index=df.index)),\n",
    "            \"beta_t\":     df.get(\"beta_t\", pd.Series(index=df.index)),\n",
    "            \"sd_beta_t\":  df.get(\"sd_beta_t\", pd.Series(index=df.index)),\n",
    "        }).dropna(subset=[\"close_bdr\",\"bdr_teo\"]).copy()\n",
    "        _save_df_to_excel(writer, \"bdr_vs_teo\", d1)\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(12,5))\n",
    "        d1[[\"close_bdr\",\"bdr_teo\",\"bdr_teo_kf\"]].dropna().plot(ax=ax1)\n",
    "        ax1.set_title(f\"{name} — BDR vs Teórico (estático & KF)\")\n",
    "        ax1.set_xlabel(\"Tempo\"); ax1.set_ylabel(\"Preço (BRL)\")\n",
    "        ax1.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        d_beta = d1[[\"beta_t\",\"sd_beta_t\"]].dropna()\n",
    "        if not d_beta.empty:\n",
    "            d_beta[\"beta_t\"].plot(ax=ax2, linestyle=\"--\")\n",
    "            d_beta[\"sd_beta_t\"].plot(ax=ax2, linestyle=\":\")\n",
    "            ax2.set_ylabel(\"β_t / sd_β_t\")\n",
    "        fig.tight_layout(); fig.savefig(report_dir / f\"{name}_bdr_vs_teo.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "        # ---------------- (2) Spread_kf & Z_kf + faixas de regime + trades ----------------\n",
    "        spread_kf_price = (df[\"close_bdr\"] - df.get(\"bdr_teo_kf\", df[\"bdr_teo\"])).rename(\"spread_kf_price\")\n",
    "        z_kf = df.get(\"z_kf\", pd.Series(index=df.index))\n",
    "        gate_kf = df.get(\"gate_kf\", pd.Series(False, index=df.index)).astype(bool)\n",
    "        gate_kf_hmm = df.get(\"gate_kf_hmm\", pd.Series(False, index=df.index)).astype(bool)\n",
    "\n",
    "        # marcar entradas/saídas/stops a partir do bt_kf (se existir)\n",
    "        entries, exits, stops = [], [], []\n",
    "        if bt_kf is not None and \"pos\" in bt_kf.columns:\n",
    "            entries, exits, stops = _mark_entries_exits_stops(bt_kf[\"pos\"], z_kf, cfg[\"z_stop\"])\n",
    "\n",
    "        d2 = pd.DataFrame({\n",
    "            \"spread_kf_price\": spread_kf_price,\n",
    "            \"z_kf\": z_kf,\n",
    "            \"gate_kf\": gate_kf,\n",
    "            \"gate_kf_hmm\": gate_kf_hmm\n",
    "        })\n",
    "        _save_df_to_excel(writer, \"spread_z_regimes\", d2)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12,6))\n",
    "        d2[\"spread_kf_price\"].dropna().plot(ax=ax)\n",
    "        ax.set_title(f\"{name} — Spread_kf & Z_kf com regimes\")\n",
    "        ax.set_xlabel(\"Tempo\"); ax.set_ylabel(\"Spread (BRL)\")\n",
    "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "        # faixas gate_kf (claro) e gate_kf_hmm (escuro)\n",
    "        ylim = ax.get_ylim()\n",
    "        on_kf = (gate_kf == True)\n",
    "        on_hmm = (gate_kf_hmm == True)\n",
    "        # blocos contínuos\n",
    "        for on, alpha in [(on_kf, 0.15), (on_hmm, 0.3)]:\n",
    "            runs = (on.astype(int).diff().fillna(on.astype(int)) != 0)\n",
    "            idx = list(on.index[runs])\n",
    "            if len(idx) == 0 and on.any():\n",
    "                idx = [on.index[0]]\n",
    "            if on.iloc[0]:\n",
    "                idx = [on.index[0]] + idx\n",
    "            # fecha blocos\n",
    "            for i in range(0, len(idx), 2):\n",
    "                start = idx[i]\n",
    "                end = idx[i+1] if i+1 < len(idx) else on.index[-1]\n",
    "                if on.loc[start]:\n",
    "                    ax.axvspan(start, end, color=\"tab:green\", alpha=alpha, lw=0)\n",
    "\n",
    "        # eixo secundário para z_kf\n",
    "        axz = ax.twinx()\n",
    "        z_kf.dropna().plot(ax=axz, linestyle=\"--\")\n",
    "        axz.set_ylabel(\"Z_kf\")\n",
    "        axz.axhline(cfg[\"z_enter\"], linestyle=\":\")\n",
    "        axz.axhline(-cfg[\"z_enter\"], linestyle=\":\")\n",
    "        axz.axhline(cfg[\"z_exit\"], linestyle=\"-.\")\n",
    "        axz.axhline(-cfg[\"z_exit\"], linestyle=\"-.\")\n",
    "        axz.axhline(cfg[\"z_stop\"], linestyle=\"--\")\n",
    "        axz.axhline(-cfg[\"z_stop\"], linestyle=\"--\")\n",
    "\n",
    "        # marcação de entradas/saídas/stops\n",
    "        for t in entries: ax.axvline(t, color=\"tab:blue\", alpha=0.5, linewidth=1.0)\n",
    "        for t in exits:   ax.axvline(t, color=\"tab:orange\", alpha=0.7, linewidth=1.0)\n",
    "        for t in stops:   ax.axvline(t, color=\"tab:red\", alpha=0.9, linewidth=1.0)\n",
    "\n",
    "        fig.tight_layout(); fig.savefig(report_dir / f\"{name}_spread_z_regimes.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "        # ---------------- (3) P(MR) e threshold (se existir) ----------------\n",
    "        if \"p_mr\" in df.columns:\n",
    "            thr = float(cfg[\"hmm_p_threshold\"])\n",
    "            d3 = pd.DataFrame({\"p_mr\": df[\"p_mr\"], \"threshold\": pd.Series(thr, index=df.index)})\n",
    "            _save_df_to_excel(writer, \"p_mr\", d3)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12,4))\n",
    "            d3[\"p_mr\"].dropna().plot(ax=ax)\n",
    "            ax.axhline(thr, linestyle=\"--\")\n",
    "            ax.set_title(f\"{name} — P(MR) filtrada (sem look-ahead)\"); ax.set_ylim(0, 1)\n",
    "            ax.set_xlabel(\"Tempo\"); ax.set_ylabel(\"P(MR)\")\n",
    "            ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "            # sombrear onde p_mr.shift(1) >= thr\n",
    "            mr_ok = (df[\"p_mr\"].shift(1) >= thr)\n",
    "            for i, v in mr_ok.groupby((mr_ok != mr_ok.shift()).cumsum()):\n",
    "                block = v[v]\n",
    "                if not block.empty:\n",
    "                    ax.axvspan(block.index[0], block.index[-1], color=\"tab:green\", alpha=0.15, lw=0)\n",
    "            fig.tight_layout(); fig.savefig(report_dir / f\"{name}_pmr.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "            # --------- (4) Matriz de transição empírica (heatmap) ----------\n",
    "            mr_flag = (df[\"p_mr\"] > 0.5)  # definição simples de estado MR\n",
    "            Aemp = _empirical_transition_matrix(mr_flag)\n",
    "            # salva em aba\n",
    "            A_df = pd.DataFrame(Aemp, index=[\"prev=NR\",\"prev=MR\"], columns=[\"NR\",\"MR\"])\n",
    "            _save_df_to_excel(writer, \"A_empirical\", A_df.reset_index().rename(columns={'index':'prev'}))\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(4,3))\n",
    "            im = ax.imshow(Aemp, aspect=\"equal\")\n",
    "            ax.set_xticks([0,1]); ax.set_xticklabels([\"NR\",\"MR\"])\n",
    "            ax.set_yticks([0,1]); ax.set_yticklabels([\"prev=NR\",\"prev=MR\"])\n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    ax.text(j, i, f\"{Aemp[i,j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "            ax.set_title(f\"{name} — Matriz de transição (empírica)\")\n",
    "            fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            fig.tight_layout(); fig.savefig(report_dir / f\"{name}_A_empirical.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "        # ---------------- (5) Equities comparadas + drawdown ----------------\n",
    "        eq_df = pd.DataFrame(index=df.index)\n",
    "        if bt_base is not None and \"equity\" in bt_base: eq_df[\"equity_base\"] = bt_base[\"equity\"]\n",
    "        if bt_kf   is not None and \"equity\" in bt_kf:   eq_df[\"equity_kf\"]   = bt_kf[\"equity\"]\n",
    "        if bt_hmm  is not None and \"equity\" in bt_hmm:  eq_df[\"equity_hmm\"]  = bt_hmm[\"equity\"]\n",
    "        dd_df = eq_df.apply(lambda s: s - s.cummax())\n",
    "        d5 = pd.concat([eq_df, dd_df.add_suffix(\"_dd\")], axis=1).dropna(how=\"all\")\n",
    "        _save_df_to_excel(writer, \"equities_dd\", d5)\n",
    "\n",
    "        if not d5.empty:\n",
    "            fig, ax = plt.subplots(figsize=(12,6))\n",
    "            eq_df.dropna(how=\"all\").plot(ax=ax)\n",
    "            ax.set_title(f\"{name} — Equity (BASE vs KF vs KF+HMM)\")\n",
    "            ax.set_xlabel(\"Tempo\"); ax.set_ylabel(\"P/L acumulado (unidades de preço)\")\n",
    "            ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "            fig.tight_layout(); fig.savefig(report_dir / f\"{name}_equities.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12,3.5))\n",
    "            dd_df.dropna(how=\"all\").plot(ax=ax)\n",
    "            ax.set_title(f\"{name} — Drawdown\")\n",
    "            ax.set_xlabel(\"Tempo\"); ax.set_ylabel(\"DD\")\n",
    "            ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "            fig.tight_layout(); fig.savefig(report_dir / f\"{name}_drawdown.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "        # ---------------- (6) Distribuição PnL por trade -------------------\n",
    "        trades_rows = []\n",
    "        def _collect_trades(bt: pd.DataFrame, label: str):\n",
    "            if bt is None or \"equity\" not in bt or \"pos\" not in bt: return\n",
    "            segments = _trade_segments_from_pos(bt[\"pos\"])\n",
    "            for (t0, t1) in segments:\n",
    "                eq = bt[\"equity\"]\n",
    "                try:\n",
    "                    pnl = float(eq.loc[t1] - eq.loc[t0])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                trades_rows.append({\"strategy\": label, \"t_in\": t0, \"t_out\": t1, \"pnl\": pnl})\n",
    "\n",
    "        _collect_trades(bt_kf, \"KF\")\n",
    "        _collect_trades(bt_hmm, \"KF+HMM\")\n",
    "\n",
    "        # bloqueados pelo HMM: trades em KF que não aparecem no KF+HMM (aprox por tempo)\n",
    "        if bt_kf is not None and bt_hmm is not None:\n",
    "            seg_kf  = _trade_segments_from_pos(bt_kf[\"pos\"])\n",
    "            seg_hmm = _trade_segments_from_pos(bt_hmm[\"pos\"])\n",
    "            hmm_intervals = pd.IntervalIndex.from_tuples([(a,b) for a,b in seg_hmm], closed=\"both\") if seg_hmm else pd.IntervalIndex([])\n",
    "            for (t0, t1) in seg_kf:\n",
    "                if not any([(t0 in iv) or (t1 in iv) or ((iv.left<=t0) and (iv.right>=t1)) for iv in hmm_intervals]):\n",
    "                    eq = bt_kf[\"equity\"]\n",
    "                    if t0 in eq.index and t1 in eq.index:\n",
    "                        pnl = float(eq.loc[t1] - eq.loc[t0])\n",
    "                        trades_rows.append({\"strategy\":\"Bloqueado_HMM\", \"t_in\": t0, \"t_out\": t1, \"pnl\": pnl})\n",
    "\n",
    "        trades_df = pd.DataFrame(trades_rows)\n",
    "        _save_df_to_excel(writer, \"trades\", trades_df)\n",
    "\n",
    "        if not trades_df.empty:\n",
    "            fig, ax = plt.subplots(figsize=(8,4))\n",
    "            # histograma simples por estratégia\n",
    "            for label, sub in trades_df.groupby(\"strategy\"):\n",
    "                ax.hist(sub[\"pnl\"].values, bins=30, alpha=0.5, label=label)\n",
    "            ax.set_title(f\"{name} — PnL por trade (estratificado)\")\n",
    "            ax.set_xlabel(\"PnL/trade\"); ax.set_ylabel(\"freq\")\n",
    "            ax.legend()\n",
    "            fig.tight_layout(); fig.savefig(report_dir / f\"{name}_trades_hist.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "        # ---------------- (7) Turnover & custos -----------------------------\n",
    "        def _turnover(bt: pd.DataFrame):\n",
    "            if bt is None or \"pos\" not in bt: return pd.Series(index=df.index, dtype=float)\n",
    "            return bt[\"pos\"].diff().abs().rename(\"turnover\")\n",
    "\n",
    "        d7 = pd.DataFrame({\n",
    "            \"turnover_kf\": _turnover(bt_kf),\n",
    "            \"turnover_hmm\": _turnover(bt_hmm),\n",
    "            \"trade_cost_kf\": bt_kf[\"trade_cost\"] if bt_kf is not None and \"trade_cost\" in bt_kf else pd.Series(index=df.index),\n",
    "            \"borrow_cost_kf\": bt_kf[\"borrow_cost\"] if bt_kf is not None and \"borrow_cost\" in bt_kf else pd.Series(index=df.index),\n",
    "            \"fx_cost_kf\": bt_kf[\"fx_cost\"] if bt_kf is not None and \"fx_cost\" in bt_kf else pd.Series(index=df.index),\n",
    "            \"trade_cost_hmm\": bt_hmm[\"trade_cost\"] if bt_hmm is not None and \"trade_cost\" in bt_hmm else pd.Series(index=df.index),\n",
    "            \"borrow_cost_hmm\": bt_hmm[\"borrow_cost\"] if bt_hmm is not None and \"borrow_cost\" in bt_hmm else pd.Series(index=df.index),\n",
    "            \"fx_cost_hmm\": bt_hmm[\"fx_cost\"] if bt_hmm is not None and \"fx_cost\" in bt_hmm else pd.Series(index=df.index),\n",
    "        })\n",
    "        _save_df_to_excel(writer, \"turnover_costs\", d7)\n",
    "\n",
    "        if not d7.dropna(how=\"all\").empty:\n",
    "            fig, ax = plt.subplots(figsize=(12,4))\n",
    "            d7[[\"turnover_kf\",\"turnover_hmm\"]].dropna(how=\"all\").plot(ax=ax)\n",
    "            ax.set_title(f\"{name} — Turnover (KF vs KF+HMM)\")\n",
    "            ax.set_xlabel(\"Tempo\"); ax.set_ylabel(\"|Δpos|\")\n",
    "            fig.tight_layout(); fig.savefig(report_dir / f\"{name}_turnover.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "            # custos empilhados (soma diária)\n",
    "            fig, ax = plt.subplots(figsize=(12,4))\n",
    "            costs_kf = d7[[\"trade_cost_kf\",\"borrow_cost_kf\",\"fx_cost_kf\"]].fillna(0.0).sum(axis=1)\n",
    "            costs_hm = d7[[\"trade_cost_hmm\",\"borrow_cost_hmm\",\"fx_cost_hmm\"]].fillna(0.0).sum(axis=1)\n",
    "            pd.DataFrame({\"KF\":costs_kf, \"KF+HMM\":costs_hm}).plot(ax=ax)\n",
    "            ax.set_title(f\"{name} — Custos (diários)\")\n",
    "            ax.set_xlabel(\"Tempo\"); ax.set_ylabel(\"Custo\")\n",
    "            fig.tight_layout(); fig.savefig(report_dir / f\"{name}_costs.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "        # ---------------- (8) Rolling Sharpe (63) ---------------------------\n",
    "        d8 = pd.DataFrame(index=df.index)\n",
    "        if bt_base is not None and \"pnl_net\" in bt_base:\n",
    "            d8[\"RS_base\"] = _rolling_sharpe(bt_base[\"pnl_net\"], 63, bt_base.index)\n",
    "        if bt_kf is not None and \"pnl_net\" in bt_kf:\n",
    "            d8[\"RS_kf\"] = _rolling_sharpe(bt_kf[\"pnl_net\"], 63, bt_kf.index)\n",
    "        if bt_hmm is not None and \"pnl_net\" in bt_hmm:\n",
    "            d8[\"RS_hmm\"] = _rolling_sharpe(bt_hmm[\"pnl_net\"], 63, bt_hmm.index)\n",
    "        _save_df_to_excel(writer, \"rolling_sharpe\", d8)\n",
    "\n",
    "        if not d8.dropna(how=\"all\").empty:\n",
    "            fig, ax = plt.subplots(figsize=(12,4))\n",
    "            d8.dropna(how=\"all\").plot(ax=ax)\n",
    "            ax.set_title(f\"{name} — Rolling Sharpe (63)\")\n",
    "            ax.set_xlabel(\"Tempo\"); ax.set_ylabel(\"Sharpe anualizado\")\n",
    "            fig.tight_layout(); fig.savefig(report_dir / f\"{name}_rolling_sharpe.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "        # ---------------- (9) Sensibilidade ao threshold do HMM ------------\n",
    "        if \"p_mr\" in df.columns and bt_kf is not None:\n",
    "            # roda backtests rápidos (reutiliza função original) variando o limiar\n",
    "            thresholds = np.arange(0.50, 0.81, 0.05)\n",
    "            results = []\n",
    "            for thr in thresholds:\n",
    "                gate = df.get(\"gate_kf\", pd.Series(False, index=df.index)).astype(bool) & (df[\"p_mr\"].shift(1) >= thr)\n",
    "                # pequeno backtest só para Sharpe\n",
    "                tmp_local = df.copy()\n",
    "                z_col = \"z_kf\" if \"z_kf\" in df.columns else \"z\"\n",
    "                tmp_local[z_col] = tmp_local[z_col].astype(float)\n",
    "                bt_tmp, sum_tmp = backtest_next_bar(\n",
    "                    tmp_local,\n",
    "                    z_col=z_col,\n",
    "                    theo_col=\"bdr_teo_kf\" if \"bdr_teo_kf\" in df.columns else \"bdr_teo\",\n",
    "                    z_entry=cfg[\"z_enter\"], z_exit=cfg[\"z_exit\"], z_stop=cfg[\"z_stop\"],\n",
    "                    slippage_bps=cfg[\"slippage_bps\"], commission_bps=cfg[\"commission_bps\"],\n",
    "                    borrow_bps_pa=cfg[\"borrow_bps_pa\"], fx_basis_bps=cfg[\"fx_basis_bps\"],\n",
    "                    use_risk_sizing=True, ewm_alpha=cfg[\"ewm_alpha\"],\n",
    "                    target_vol=cfg[\"target_vol\"], max_leverage=cfg[\"max_leverage\"],\n",
    "                    gate=gate, sd_beta=df.get(\"sd_beta_t\"), x_log=np.log(df[\"bdr_teo\"].clip(lower=1e-12)),\n",
    "                    uncertainty_weight=cfg[\"uncertainty_weight\"]\n",
    "                )\n",
    "                results.append({\"threshold\": round(float(thr),2), \"sharpe\": sum_tmp[\"sharpe_hourly_annualized\"]})\n",
    "            sens_df = pd.DataFrame(results)\n",
    "            _save_df_to_excel(writer, \"sens_threshold\", sens_df)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(6,4))\n",
    "            ax.plot(sens_df[\"threshold\"], sens_df[\"sharpe\"], marker=\"o\")\n",
    "            ax.set_title(f\"{name} — Sensibilidade do threshold do HMM\")\n",
    "            ax.set_xlabel(\"threshold\"); ax.set_ylabel(\"Sharpe anualizado\")\n",
    "            fig.tight_layout(); fig.savefig(report_dir / f\"{name}_sens_threshold.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "        # ---------------- (10) Metadados/params -----------------------------\n",
    "        meta = pd.DataFrame({\n",
    "            \"key\": list(cfg.keys()),\n",
    "            \"value\": [str(cfg[k]) for k in cfg.keys()]\n",
    "        })\n",
    "        _save_df_to_excel(writer, \"params_cfg\", meta)\n",
    "\n",
    "    # Log final\n",
    "    log.info(f\"[reports] Gráficos e Excel salvos em: {report_dir}\")\n",
    "    return report_dir\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "def main():\n",
    "    OUT_DIR = Path(CFG[\"out_dir\"])\n",
    "    (OUT_DIR / \"pairs\").mkdir(parents=True, exist_ok=True)\n",
    "    (OUT_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Carrega dados\n",
    "    log.info(\"Lendo BDRs…\")\n",
    "    bdr_map = load_bdr_sheets(Path(CFG[\"path_bdr_xlsx\"]))\n",
    "    log.info(f\"BDRs lidas: {len(bdr_map)}\")\n",
    "\n",
    "    log.info(\"Lendo ADRs…\")\n",
    "    us_map  = load_us_sheets(Path(CFG[\"path_us_xlsx\"]))\n",
    "    log.info(f\"ADRs lidas: {len(us_map)}\")\n",
    "\n",
    "    log.info(\"Lendo FX (USDBRL)…\")\n",
    "    fx_df   = load_fx(Path(CFG[\"path_fx_xlsx\"]))\n",
    "\n",
    "    # Loop de pares\n",
    "    rows_summary = []\n",
    "    for bdr, adr in CFG[\"pairs\"].items():\n",
    "        if bdr not in bdr_map:\n",
    "            log.warning(f\"BDR '{bdr}' não encontrado; pulando.\")\n",
    "            continue\n",
    "        if adr not in us_map:\n",
    "            log.warning(f\"ADR '{adr}' não encontrado; pulando.\")\n",
    "            continue\n",
    "\n",
    "        res = run_pipeline_one_pair(\n",
    "            bdr, adr, bdr_map, us_map, fx_df,\n",
    "            out_pairs_dir=(OUT_DIR / \"pairs\"),\n",
    "            plots_dir=(OUT_DIR / \"plots\")\n",
    "        )\n",
    "        if res is None:\n",
    "            continue\n",
    "        summary, pair_parquet_path = res\n",
    "        rows_summary.append(summary)\n",
    "        try:\n",
    "            make_pair_report(pair_parquet_path, OUT_DIR, bdr, adr, CFG)\n",
    "        except Exception as e:\n",
    "           log.exception(f\"Falha ao gerar report de {bdr}_{adr}: {e}\")\n",
    "\n",
    "    # Consolidado\n",
    "    if rows_summary:\n",
    "        df_sum = pd.DataFrame(rows_summary).sort_values(\"sharpe_kf\", ascending=False)\n",
    "        df_sum.to_csv(OUT_DIR / \"summary_pairs.csv\", index=False, float_format=\"%.8f\")\n",
    "        log.info(\"\\n== Top 10 por Sharpe (KF) ==\")\n",
    "        cols = [\"bdr\",\"adr\",\"overlap_obs\",\"mae_teorico\",\"mae_kf\",\n",
    "                \"sharpe_base\",\"pnl_base\",\"trades_base\",\n",
    "                \"sharpe_kf\",\"pnl_kf\",\"trades_kf\",\"mdd_kf\"]\n",
    "        if CFG.get(\"use_hmm_gate\", False):\n",
    "            cols += [\"sharpe_kf_hmm\",\"pnl_kf_hmm\",\"trades_kf_hmm\",\"mdd_kf_hmm\"]\n",
    "        # imprime só colunas que existem (caso algum par não tenha HMM por falta de dados)\n",
    "        cols = [c for c in cols if c in df_sum.columns]\n",
    "        log.info(\"\\n\" + df_sum[cols].head(10).to_string(index=False))\n",
    "        \n",
    "        # Gráfico e Excel do ranking por par (por Sharpe KF / KF+HMM se existir)\n",
    "        rank_dir = OUT_DIR / \"reports\" / \"ranking\"\n",
    "        rank_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df_plot = df_sum.copy()\n",
    "        metric = \"sharpe_kf_hmm\" if \"sharpe_kf_hmm\" in df_plot.columns else \"sharpe_kf\"\n",
    "        df_plot = df_plot.sort_values(metric, ascending=True)\n",
    "\n",
    "        # salva Excel com exatamente as colunas do gráfico\n",
    "        df_plot_out = df_plot[[\"bdr\",\"adr\",metric,\"mdd_kf\" if metric==\"sharpe_kf\" else \"mdd_kf_hmm\"]].rename(\n",
    "            columns={metric:\"sharpe\", \"mdd_kf\":\"mdd\", \"mdd_kf_hmm\":\"mdd\"}\n",
    "        )\n",
    "        df_plot_out.to_excel(rank_dir / \"ranking_data_for_plot.xlsx\", index=False)\n",
    "\n",
    "        plt.figure(figsize=(8, max(4, len(df_plot_out)*0.4)))\n",
    "        plt.barh(df_plot_out[\"bdr\"]+\"_\"+df_plot_out[\"adr\"], df_plot_out[\"sharpe\"])\n",
    "        plt.title(f\"Ranking por par — {metric.upper()}\")\n",
    "        plt.xlabel(\"Sharpe anualizado\"); plt.tight_layout()\n",
    "        plt.savefig(rank_dir / \"ranking_sharpe.png\", dpi=160); plt.close()\n",
    "\n",
    "    else:\n",
    "        log.warning(\"Nenhum par válido processado.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
